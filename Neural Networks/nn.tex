\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}

\geometry{margin=1in}

\title{\textbf{Neural Networks}}
\author{}
\date{}

\begin{document}
\maketitle

\tableofcontents
\newpage


%=================================================
\section{Neural Networks}
%=================================================

Neural Networks were not invented to imitate the brain.
They were invented to solve a precise mathematical problem:

\begin{quote}
How do we represent and learn nonlinear functions from data?
\end{quote}

To understand neural networks, we must first understand
\textbf{why linear models fundamentally fail}.

%=================================================
\section{NN--Module 1: Why Neural Networks Exist}
%=================================================

\subsection*{The Learning Problem Revisited}

We observe data:
\[
(x_i, y_i), \quad x_i \in \mathbb{R}^d
\]

Assume data is generated by:
\[
y = f(x) + \varepsilon
\]

where:
\begin{itemize}
    \item $f(x)$ is an unknown function
    \item $\varepsilon$ is noise
\end{itemize}

Learning means approximating $f(x)$.

\subsubsection*{Key Question}

\begin{quote}
What class of functions should we search in?
\end{quote}

This choice determines everything.

%=================================================
\subsection{Limits of Linear Models}
%=================================================

A linear model assumes:
\[
f(x) = w^T x + b
\]

This represents a hyperplane in $\mathbb{R}^d$.

\subsubsection*{Geometric Meaning}

\begin{itemize}
    \item Linear models create flat decision boundaries
    \item Classification is separation by a hyperplane
    \item Regression is fitting a flat surface
\end{itemize}

Linear models are powerful — but limited.

%-
\subsubsection*{Strang Bridge — What Linear Models Can and Cannot Do}

Linear models can:
\begin{itemize}
    \item Interpolate smoothly
    \item Generalize well with limited data
\end{itemize}

Linear models cannot:
\begin{itemize}
    \item Bend decision boundaries
    \item Represent interactions naturally
\end{itemize}

This limitation is structural, not algorithmic.

%=================================================
\subsection{The XOR Problem (Canonical Failure)}
%=================================================

Consider binary inputs:
\[
x = (x_1, x_2) \in \{0,1\}^2
\]

Define XOR:
\[
y = x_1 \oplus x_2
\]

Truth table:
\[
\begin{array}{c|c|c}
x_1 & x_2 & y \\
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0
\end{array}
\]

%-
\subsubsection*{Geometric View}

Plotting points in $\mathbb{R}^2$:
\begin{itemize}
    \item Class 1: $(0,1), (1,0)$
    \item Class 0: $(0,0), (1,1)$
\end{itemize}

\textbf{There exists no straight line that separates these two classes.}

%-
\subsubsection*{Formal Proof of Impossibility}

Assume a linear classifier:
\[
y = \text{sign}(w_1 x_1 + w_2 x_2 + b)
\]

Correct classification requires:
\[
\begin{aligned}
b &< 0 \quad &\text{(for $(0,0)$)} \\
w_1 + b &> 0 \quad &\text{(for $(1,0)$)} \\
w_2 + b &> 0 \quad &\text{(for $(0,1)$)} \\
w_1 + w_2 + b &< 0 \quad &\text{(for $(1,1)$)}
\end{aligned}
\]

Adding the middle two inequalities:
\[
w_1 + w_2 + 2b > 0
\]

But from the last inequality:
\[
w_1 + w_2 + b < 0
\]

Subtracting:
\[
b > 0
\]

Contradiction with $b < 0$.

\subsubsection*{Conclusion}

XOR cannot be represented by any linear model.

%=================================================
\subsection{What Exactly Is Missing?}
%=================================================

The problem is \textbf{not} lack of parameters.

The problem is lack of \textbf{nonlinearity}.

\subsubsection*{Key Insight}

A sum of linear functions is still linear:
\[
\sum_i a_i (w_i^T x + b_i) = W^T x + c
\]

No matter how many linear layers we stack,
without nonlinearity, nothing new is created.

%=================================================
\subsection{Introducing Nonlinearity}
%=================================================

Suppose we apply a nonlinear function $\sigma(\cdot)$:
\[
f(x) = \sigma(w^T x + b)
\]

This immediately changes the function class.

\subsubsection*{Geometric Meaning}

\begin{itemize}
    \item $w^T x + b$ defines a hyperplane
    \item $\sigma(\cdot)$ bends space across that hyperplane
\end{itemize}

This is the birth of a \textbf{neuron}.

%=================================================
\subsection{Composition of Functions}
%=================================================

Neural networks are compositions:
\[
f(x) = f_L \circ f_{L-1} \circ \cdots \circ f_1(x)
\]

Each layer:
\[
f_\ell(x) = \sigma(W_\ell x + b_\ell)
\]

\subsubsection*{Strang Bridge — Why Composition Matters}

A single nonlinear transformation bends space once.
Multiple layers bend space repeatedly.

This allows:
\begin{itemize}
    \item Folding
    \item Warping
    \item Complex decision regions
\end{itemize}

%=================================================
\subsection{Universal Approximation (Idea Only)}
%=================================================

A neural network with:
\begin{itemize}
    \item One hidden layer
    \item Enough neurons
\end{itemize}

can approximate any continuous function on a compact set.

\subsubsection*{Important Clarification}

Universal approximation means:
\begin{itemize}
    \item Existence, not efficiency
    \item Approximation, not exact equality
\end{itemize}

Depth improves efficiency.

%=================================================
\subsection{Why Depth Helps}
%=================================================

Shallow networks:
\begin{itemize}
    \item Need exponentially many neurons
\end{itemize}

Deep networks:
\begin{itemize}
    \item Reuse features
    \item Build hierarchical representations
\end{itemize}

\subsubsection*{Strang Bridge — Linear Algebra Analogy}

Matrix multiplication:
\[
A = A_3 A_2 A_1
\]

Composition is powerful.

Neural networks are nonlinear analogues of this idea.

%=================================================
\subsection{Summary of Module NN--1}
%=================================================

\begin{itemize}
    \item Linear models are geometrically limited
    \item XOR exposes this limitation clearly
    \item Nonlinearity is essential
    \item Neural networks arise from composing nonlinear maps
\end{itemize}

This motivates everything that follows.

%=================================================
\section{NN--Module 2: The Single Neuron (Perceptron)}
%=================================================

In this module, we study the simplest nonlinear building block
of a neural network: the \textbf{neuron}.

A neuron is not biology.
It is a precise mathematical object.

%=================================================
\subsection{From Linear Model to Neuron}
%=================================================

Recall the linear model:
\[
f(x) = w^T x + b
\]

This computes a scalar score.
By itself, this score has no decision meaning.

To make a decision, we apply a nonlinear function.

\subsubsection*{Neuron Definition}

A neuron computes:
\[
f(x) = \sigma(w^T x + b)
\]

where:
\begin{itemize}
    \item $x \in \mathbb{R}^d$ is the input
    \item $w \in \mathbb{R}^d$ is the weight vector
    \item $b \in \mathbb{R}$ is the bias
    \item $\sigma(\cdot)$ is an activation function
\end{itemize}

This is the atomic unit of a neural network.

%=================================================
\subsection{Geometry of the Affine Map}
%=================================================

Consider the affine function:
\[
z(x) = w^T x + b
\]

\subsubsection*{Hyperplane Geometry}

The set:
\[
w^T x + b = 0
\]

defines a hyperplane in $\mathbb{R}^d$.

This hyperplane:
\begin{itemize}
    \item Divides space into two half-spaces
    \item Has normal vector $w$
\end{itemize}

\subsubsection*{Signed Distance Interpretation}

The signed distance of a point $x$ from the hyperplane is:
\[
\frac{w^T x + b}{\|w\|}
\]

Thus:
\begin{itemize}
    \item $w$ controls orientation
    \item $b$ controls translation
\end{itemize}

%=================================================
\subsection{Activation Functions: Why Nonlinearity?}
%=================================================

Without $\sigma(\cdot)$, stacking neurons gives:
\[
W_2(W_1 x + b_1) + b_2 = W x + c
\]

This is still linear.

\subsubsection*{Key Principle}

\begin{quote}
Nonlinearity is not optional.
It is what gives expressive power.
\end{quote}

%=================================================
\subsection{The Step Function (Perceptron)}
%=================================================

The earliest neuron used the step function:
\[
\sigma(z) =
\begin{cases}
1, & z \ge 0 \\
0, & z < 0
\end{cases}
\]

Thus:
\[
f(x) = \mathbb{I}(w^T x + b \ge 0)
\]

This is the \textbf{perceptron}.

%=================================================
\subsection{Perceptron as a Classifier}
%=================================================

The perceptron implements:
\[
\text{class}(x) =
\begin{cases}
+1, & w^T x + b \ge 0 \\
-1, & w^T x + b < 0
\end{cases}
\]

This is equivalent to:
\[
\text{sign}(w^T x + b)
\]

\subsubsection*{Geometric Meaning}

The neuron:
\begin{itemize}
    \item Classifies based on which side of a hyperplane $x$ lies
    \item Is a linear classifier
\end{itemize}

%=================================================
\subsection{What Can a Single Neuron Represent?}
%=================================================

A single neuron can represent:
\begin{itemize}
    \item AND
    \item OR
    \item NAND
\end{itemize}

but cannot represent XOR.

\subsubsection*{Explicit Example: AND Gate}

Let:
\[
w = \begin{bmatrix}1 \\ 1\end{bmatrix}, \quad b = -1.5
\]

Then:
\[
w^T x + b \ge 0
\quad \Longleftrightarrow \quad
x_1 = x_2 = 1
\]

Thus AND is linearly separable.

%=================================================
\subsection{Perceptron Learning Rule}
%=================================================

Given labeled data:
\[
(x_i, y_i), \quad y_i \in \{-1,+1\}
\]

Prediction:
\[
\hat{y}_i = \text{sign}(w^T x_i + b)
\]

If a point is misclassified:
\[
y_i(w^T x_i + b) \le 0
\]

update:
\[
w \leftarrow w + \eta y_i x_i
\]
\[
b \leftarrow b + \eta y_i
\]

where $\eta > 0$ is the learning rate.

%=================================================
\subsection{Why the Update Makes Sense (Geometry)}
%=================================================

If $y_i = +1$ and prediction is wrong:
\begin{itemize}
    \item $w^T x_i + b < 0$
    \item Adding $x_i$ moves the hyperplane toward correct side
\end{itemize}

If $y_i = -1$ and prediction is wrong:
\begin{itemize}
    \item Subtracting $x_i$ pushes hyperplane away
\end{itemize}

\subsubsection*{Key Insight}

The update moves the hyperplane
to reduce future misclassification.

%=================================================
\subsection{Perceptron Convergence Theorem}
%=================================================

\textbf{Theorem}

If the data is linearly separable,
the perceptron learning algorithm converges
in a finite number of steps.

\subsubsection*{Important Consequence}

\begin{itemize}
    \item Convergence is guaranteed
    \item Only if linear separation exists
\end{itemize}

XOR fails this condition.

%=================================================
\subsection{Limitations of the Perceptron}
%=================================================

\begin{itemize}
    \item Cannot learn nonlinear decision boundaries
    \item Step function is non-differentiable
    \item No probabilistic interpretation
\end{itemize}

These limitations motivate smooth activations.

%=================================================
\subsection{Smooth Activations: Sigmoid and Tanh}
%=================================================

\subsubsection*{Sigmoid}

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

\begin{itemize}
    \item Smooth approximation to step
    \item Differentiable everywhere
\end{itemize}

\subsubsection*{Tanh}

\[
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]

\begin{itemize}
    \item Zero-centered output
    \item Steeper gradients near zero
\end{itemize}

%=================================================
\subsection{Neuron as a Function Approximator}
%=================================================

A single neuron computes:
\[
f(x) = \sigma(w^T x + b)
\]

This is:
\begin{itemize}
    \item A nonlinear function
    \item With one degree of bending
\end{itemize}

Multiple neurons are required for complex shapes.

%=================================================
\subsection{Summary of Module NN--2}
%=================================================

\begin{itemize}
    \item A neuron = affine map + nonlinearity
    \item Geometry is governed by hyperplanes
    \item Perceptron learns linear separators
    \item Non-differentiability limits learning
    \item Smooth activations enable calculuss
\end{itemize}

This prepares us for multi-layer networks.
%=================================================
\section{NN--Module 3: Multi-Layer Perceptron (MLP)}
%=================================================

A single neuron can only create one nonlinear bend.
To represent complex functions, neurons must be composed.

This composition is called a \textbf{Multi-Layer Perceptron (MLP)}.

%=================================================
\subsection{From One Neuron to Many}
%=================================================

Consider a single neuron:
\[
f(x) = \sigma(w^T x + b)
\]

Now consider multiple neurons acting in parallel.

\subsubsection*{Hidden Layer Construction}

Let there be $m$ neurons in a hidden layer.
Each neuron has weights $w_j$ and bias $b_j$.

The hidden layer computes:
\[
h(x) =
\begin{bmatrix}
\sigma(w_1^T x + b_1) \\
\sigma(w_2^T x + b_2) \\
\vdots \\
\sigma(w_m^T x + b_m)
\end{bmatrix}
\in \mathbb{R}^m
\]

This is a vector-valued function.

%=================================================
\subsection{Matrix Representation (Critical)}
%=================================================

Define:
\[
W =
\begin{bmatrix}
w_1^T \\
w_2^T \\
\vdots \\
w_m^T
\end{bmatrix}
\in \mathbb{R}^{m \times d},
\quad
b =
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{bmatrix}
\]

Then the hidden layer computes:
\[
h(x) = \sigma(Wx + b)
\]

where $\sigma(\cdot)$ is applied elementwise.

\subsubsection*{Strang Bridge — Why Matrices Appear}

Matrices allow us to:
\begin{quote}
Apply many hyperplanes to the same input simultaneously.
\end{quote}

Each row of $W$ defines one hyperplane.

%=================================================
\subsection{Adding an Output Layer}
%=================================================

Let the output layer be linear:
\[
f(x) = v^T h(x) + c
\]

Substituting:
\[
f(x) = v^T \sigma(Wx + b) + c
\]

This is a two-layer neural network.

%=================================================
\subsection{General $L$-Layer Network}
%=================================================

An $L$-layer neural network is a composition:
\[
f(x) = f_L \circ f_{L-1} \circ \cdots \circ f_1(x)
\]

Each layer:
\[
f_\ell(z) = \sigma(W_\ell z + b_\ell)
\]

Input:
\[
z_0 = x
\]

Output:
\[
z_L = f(x)
\]

%=================================================
\subsection{Forward Propagation (Formal Definition)}
%=================================================

Forward propagation computes outputs layer by layer.

For $\ell = 1,2,\dots,L$:
\[
\begin{aligned}
a_\ell &= W_\ell z_{\ell-1} + b_\ell \\
z_\ell &= \sigma(a_\ell)
\end{aligned}
\]

where:
\begin{itemize}
    \item $a_\ell$ = pre-activation
    \item $z_\ell$ = activation
\end{itemize}

\subsubsection*{Key Observation}

Forward propagation is:
\begin{itemize}
    \item Deterministic
    \item Pure function evaluation
\end{itemize}

No learning happens here.

%=================================================
\subsection{Geometry of a Hidden Layer}
%=================================================

Each neuron defines a half-space.
A hidden layer:
\begin{itemize}
    \item Partitions space into regions
    \item Applies nonlinear warping within regions
\end{itemize}

\subsubsection*{Strang Bridge — Space Folding}

Think of space as a sheet.
Each layer folds the sheet.
Multiple folds create complex shapes.

%=================================================
\subsection{Expressivity Through Depth}
%=================================================

Depth allows:
\begin{itemize}
    \item Reuse of intermediate features
    \item Hierarchical representations
\end{itemize}

Shallow networks require exponentially many neurons
to represent functions that deep networks represent compactly.

\subsubsection*{Important Insight}

Depth is not about more parameters.
It is about \textbf{structured composition}.

%=================================================
\subsection{Output Interpretation}
%=================================================

For regression:
\[
f(x) \in \mathbb{R}
\]

For binary classification:
\[
f(x) = \sigma(a_L)
\]

For multi-class classification:
\[
f(x) = \text{softmax}(a_L)
\]

\subsubsection*{Softmax}

Given $K$ classes:
\[
\text{softmax}(a)_k =
\frac{e^{a_k}}{\sum_{j=1}^K e^{a_j}}
\]

\begin{itemize}
    \item Outputs probabilities
    \item Sum to 1
\end{itemize}

%=================================================
\subsection{Loss Functions}
%=================================================

Learning requires a measure of error.

\subsubsection*{Mean Squared Error (Regression)}

\[
L(y,\hat{y}) = \frac{1}{2}(y - \hat{y})^2
\]

\subsubsection*{Binary Cross-Entropy}

\[
L(y,\hat{y}) =
- \left[
y\log \hat{y} + (1-y)\log(1-\hat{y})
\right]
\]

\subsubsection*{Categorical Cross-Entropy}

\[
L(y,\hat{y}) =
- \sum_{k=1}^K y_k \log \hat{y}_k
\]

\subsubsection*{Strang Bridge — Why Different Losses?}

The loss must match:
\begin{itemize}
    \item Output interpretation
    \item Probability model
\end{itemize}

%=================================================
\subsection{Network as a Parametric Function}
%=================================================

An MLP defines:
\[
f(x;\theta)
\]

where:
\[
\theta = \{W_\ell, b_\ell\}_{\ell=1}^L
\]

Learning means:
\[
\min_\theta \sum_i L(y_i, f(x_i;\theta))
\]

This is a high-dimensional optimization problem.

%=================================================
\subsection{What Has Not Happened Yet}
%=================================================

At this point:
\begin{itemize}
    \item We can compute outputs
    \item We can define loss
\end{itemize}

But we do not yet know:
\begin{itemize}
    \item How to compute gradients efficiently
    \item How parameters should be updated
\end{itemize}

That is the role of backpropagation.

%=================================================
\subsection{Summary of Module NN--3}
%=================================================

\begin{itemize}
    \item MLP = composition of affine maps and nonlinearities
    \item Forward propagation is structured function evaluation
    \item Depth increases expressive power
    \item Loss connects prediction to learning
\end{itemize}

We are now ready for calculus.
%=================================================
\section{NN--Module 4: Backpropagation}
%=================================================

Backpropagation is not an algorithm.
It is a systematic application of the \textbf{chain rule}.

Its purpose is simple:

\begin{quote}
Efficiently compute gradients of a composed function.
\end{quote}

Nothing more. Nothing less.

%=================================================
\subsection{Why Gradients Are Needed}
%=================================================

Recall that learning means minimizing loss:
\[
\min_\theta \sum_{i=1}^n L(y_i, f(x_i;\theta))
\]

To minimize this using gradient-based methods,
we must compute:
\[
\frac{\partial L}{\partial \theta}
\]

where $\theta$ includes all weights and biases.

\subsubsection*{The Difficulty}

A neural network is a deeply nested function:
\[
L(y, f(x)) = L\left(y, f_L(f_{L-1}(\cdots f_1(x)))\right)
\]

Direct differentiation is inefficient unless structure is exploited.

%=================================================
\subsection{Chain Rule (Single Variable)}
%=================================================

If:
\[
z = g(u), \quad u = h(x)
\]

then:
\[
\frac{dz}{dx} = \frac{dz}{du}\frac{du}{dx}
\]

This is the chain rule.

Backpropagation is the multi-variable version of this rule.

%=================================================
\subsection{Chain Rule (Vector Form)}
%=================================================

Let:
\[
z = g(u), \quad u \in \mathbb{R}^m
\]

Then:
\[
\frac{\partial z}{\partial x}
=
\frac{\partial z}{\partial u}
\frac{\partial u}{\partial x}
\]

Jacobians multiply.

\subsubsection*{Key Insight}

Gradients flow backward through the network.

%=================================================
\subsection{Computational Graph View}
%=================================================

A neural network can be represented as a directed acyclic graph.

Each node:
\begin{itemize}
    \item Computes a simple operation
    \item Stores intermediate values
\end{itemize}

Edges represent dependencies.

\subsubsection*{Strang Bridge — Why Graphs Matter}

Graphs expose structure.
Structure enables efficient gradient computation.

%=================================================
\subsection{Backpropagation Setup}
%=================================================

Consider an $L$-layer network.

Forward pass:
\[
\begin{aligned}
a_\ell &= W_\ell z_{\ell-1} + b_\ell \\
z_\ell &= \sigma(a_\ell)
\end{aligned}
\]

Loss:
\[
L = L(y, z_L)
\]

Goal:
\[
\frac{\partial L}{\partial W_\ell}, \quad
\frac{\partial L}{\partial b_\ell}
\]

%=================================================
\subsection{Gradient at the Output Layer}
%=================================================

Define the error signal:
\[
\delta_L = \frac{\partial L}{\partial a_L}
\]

Using chain rule:
\[
\delta_L =
\frac{\partial L}{\partial z_L}
\odot
\sigma'(a_L)
\]

where $\odot$ denotes elementwise multiplication.

\subsubsection*{Interpretation}

$\delta_L$ measures how sensitive the loss is
to changes in the pre-activation $a_L$.

%=================================================
\subsection{Backpropagation Recursion}
%=================================================

For layer $\ell = L-1, \dots, 1$:

\[
\delta_\ell =
(W_{\ell+1}^T \delta_{\ell+1})
\odot
\sigma'(a_\ell)
\]

This is the backpropagation equation.

\subsubsection*{Why This Works}

\begin{itemize}
    \item $W_{\ell+1}^T$ propagates error backward
    \item $\sigma'(a_\ell)$ accounts for nonlinearity
\end{itemize}

%=================================================
\subsection{Gradients of Parameters}
%=================================================

Once $\delta_\ell$ is known:

\subsubsection*{Gradient w.r.t.\ Weights}

\[
\frac{\partial L}{\partial W_\ell}
=
\delta_\ell z_{\ell-1}^T
\]

\subsubsection*{Gradient w.r.t.\ Biases}

\[
\frac{\partial L}{\partial b_\ell}
=
\delta_\ell
\]

\subsubsection*{Strang Bridge — Outer Product Structure}

Weight gradients are outer products:
\begin{itemize}
    \item Error signal $\times$ input signal
\end{itemize}

This explains the matrix shapes.

%=================================================
\subsection{Complete Backpropagation Algorithm}
%=================================================

\begin{enumerate}
    \item Forward pass: compute all $a_\ell, z_\ell$
    \item Compute $\delta_L$
    \item For $\ell = L-1$ down to $1$, compute $\delta_\ell$
    \item Compute gradients for all $W_\ell, b_\ell$
\end{enumerate}

Time complexity is linear in number of parameters.

%=================================================
\subsection{Why Backpropagation Is Efficient}
%=================================================

Naive differentiation repeats computations.

Backpropagation:
\begin{itemize}
    \item Reuses intermediate results
    \item Avoids exponential blow-up
\end{itemize}

This is dynamic programming applied to calculus.

%=================================================
\subsection{Example: One Hidden Layer Network}
%=================================================

For:
\[
f(x) = v^T \sigma(Wx + b)
\]

Gradients are:
\[
\delta_2 = \frac{\partial L}{\partial f}
\]

\[
\delta_1 =
(v \delta_2)
\odot
\sigma'(Wx + b)
\]

\[
\frac{\partial L}{\partial W}
=
\delta_1 x^T
\]

\subsubsection*{Key Point}

Every gradient follows the same pattern.

%=================================================
\subsection{Common Misconceptions}
%=================================================

\begin{itemize}
    \item Backprop is not gradient descent
    \item Backprop is not biologically realistic
    \item Backprop is not restricted to neural networks
\end{itemize}

It is a general differentiation technique.

%=================================================
\subsection{Numerical Stability}
%=================================================

Problems arise when:
\begin{itemize}
    \item $\sigma'(a)$ is very small (vanishing gradients)
    \item $\sigma'(a)$ is very large (exploding gradients)
\end{itemize}

This motivates careful activation choice.

%=================================================
\subsection{Summary of Module NN--4}
%=================================================

\begin{itemize}
    \item Backpropagation is chain rule on graphs
    \item Error signals propagate backward
    \item Gradients have simple outer-product form
    \item Efficiency comes from reuse of computations
\end{itemize}

This completes the mathematical core.
%=================================================
\section{NN--Module 5: Optimization for Neural Networks}
%=================================================

In previous modules, we constructed a neural network
and derived exact gradients using backpropagation.

Now we ask the decisive question:

\begin{quote}
How do we actually use these gradients to learn?
\end{quote}

Optimization is where neural networks either succeed or fail.

%=================================================
\subsection{The Optimization Problem}
%=================================================

Recall the learning objective:
\[
\min_{\theta}
\; \mathcal{L}(\theta)
=
\min_{\theta}
\sum_{i=1}^n L(y_i, f(x_i;\theta))
\]

where:
\begin{itemize}
    \item $\theta$ = all weights and biases
    \item $\mathcal{L}$ is a highly non-convex function
\end{itemize}

\subsubsection*{Key Difference from Linear Models}

\begin{itemize}
    \item Linear regression: convex loss
    \item Neural networks: non-convex loss
\end{itemize}

Global minima are not guaranteed.

%=================================================
\subsection{Gradient Descent (First Principles)}
%=================================================

Let $\theta \in \mathbb{R}^p$ be parameters.

Taylor expansion:
\[
\mathcal{L}(\theta + \Delta\theta)
\approx
\mathcal{L}(\theta)
+
\nabla \mathcal{L}(\theta)^T \Delta\theta
\]

To decrease loss, choose:
\[
\Delta\theta = -\eta \nabla \mathcal{L}(\theta)
\]

Thus:
\[
\theta^{(t+1)} =
\theta^{(t)} - \eta \nabla \mathcal{L}(\theta^{(t)})
\]

This is \textbf{gradient descent}.

\subsubsection*{Strang Bridge — Why This Direction?}

The gradient points in the direction of steepest increase.
Moving opposite gives steepest decrease.

Pure geometry.

%=================================================
\subsection{Learning Rate ($\eta$)}
%=================================================

The learning rate controls step size.

\begin{itemize}
    \item Too small $\Rightarrow$ very slow learning
    \item Too large $\Rightarrow$ divergence or oscillation
\end{itemize}

\subsubsection*{Geometric Interpretation}

Loss surface:
\begin{itemize}
    \item Narrow valleys
    \item Flat plateaus
    \item Curved ravines
\end{itemize}

A fixed step size cannot be optimal everywhere.

%=================================================
\subsection{Batch vs Stochastic Gradient Descent}
%=================================================

\subsubsection*{Batch Gradient Descent}

Uses full dataset:
\[
\nabla \mathcal{L}(\theta)
=
\frac{1}{n} \sum_{i=1}^n \nabla L_i(\theta)
\]

\begin{itemize}
    \item Accurate gradient
    \item Computationally expensive
\end{itemize}

%-
\subsubsection*{Stochastic Gradient Descent (SGD)}

Uses one data point:
\[
\theta \leftarrow \theta - \eta \nabla L_i(\theta)
\]

\begin{itemize}
    \item Noisy gradient
    \item Fast updates
\end{itemize}

%-
\subsubsection*{Mini-Batch Gradient Descent}

Compromise:
\[
\nabla \mathcal{L}_B(\theta)
=
\frac{1}{|B|}
\sum_{i \in B} \nabla L_i(\theta)
\]

This is used in practice.

%=================================================
\subsection{Why Noise Helps (Important Insight)}
%=================================================

SGD noise:
\begin{itemize}
    \item Escapes shallow local minima
    \item Escapes saddle points
\end{itemize}

\subsubsection*{Strang Bridge — Physical Analogy}

Noise acts like thermal energy,
allowing the system to move across flat regions.

%=================================================
\subsection{Loss Surface Geometry}
%=================================================

Neural network loss surfaces have:
\begin{itemize}
    \item Many saddle points
    \item Flat directions
    \item Equivalent minima (symmetry)
\end{itemize}

Most critical points are saddles, not local minima.

%=================================================
\subsection{Vanishing and Exploding Gradients}
%=================================================

Backpropagation multiplies derivatives:
\[
\delta_\ell \sim
\prod_{k=\ell+1}^L
W_k^T \sigma'(a_k)
\]

\subsubsection*{Vanishing Gradients}

If:
\[
|\sigma'(a)| < 1
\]

Repeated multiplication $\Rightarrow$ gradients $\to 0$.

\subsubsection*{Exploding Gradients}

If:
\[
|\sigma'(a)| > 1
\]

Repeated multiplication $\Rightarrow$ gradients $\to \infty$.

%=================================================
\subsection{Role of Activation Functions}
%=================================================

\subsubsection*{Sigmoid / Tanh}

\begin{itemize}
    \item Saturate at extremes
    \item $\sigma'(z) \approx 0$ for large $|z|$
\end{itemize}

Cause vanishing gradients.

\subsubsection*{ReLU}

\[
\sigma(z) = \max(0,z)
\]

\begin{itemize}
    \item Constant gradient for $z>0$
    \item No saturation
\end{itemize}

ReLU alleviates vanishing gradients.

%=================================================
\subsection{Initialization Matters}
%=================================================

If weights are too small:
\begin{itemize}
    \item Activations shrink
    \item Gradients vanish
\end{itemize}

If weights are too large:
\begin{itemize}
    \item Activations explode
    \item Gradients explode
\end{itemize}

\subsubsection*{Variance Preservation Principle}

Initialization aims to keep:
\[
\text{Var}(z_\ell) \approx \text{Var}(z_{\ell-1})
\]

This motivates:
\begin{itemize}
    \item Xavier initialization
    \item He initialization
\end{itemize}

%=================================================
\subsection{Gradient Descent Variants (Idea Level)}
%=================================================

\begin{itemize}
    \item Momentum: smooths updates
    \item RMSProp: adapts learning rate
    \item Adam: combines momentum + scaling
\end{itemize}

All modify:
\[
\theta \leftarrow \theta - \text{effective step}
\]

Core principle remains gradient descent.

%=================================================
\subsection{What Optimization Can and Cannot Fix}
%=================================================

Optimization can fix:
\begin{itemize}
    \item Slow convergence
    \item Noisy updates
\end{itemize}

Optimization cannot fix:
\begin{itemize}
    \item Poor architecture
    \item Lack of data
    \item Wrong loss function
\end{itemize}

%=================================================
\subsection{Summary of Module NN--5}
%=================================================

\begin{itemize}
    \item Learning is gradient-based optimization
    \item Learning rate controls stability
    \item SGD noise is beneficial
    \item Vanishing/exploding gradients are structural
    \item Initialization and activations are critical
\end{itemize}

Optimization explains why training fails or succeeds.
%=================================================
\section{NN--Module 6: Regularization and Generalization}
%=================================================

A neural network with enough parameters can fit almost any dataset.

This creates a paradox:

\begin{quote}
Why does a highly flexible model not always overfit?
\end{quote}

This module explains the mathematics behind generalization.

%=================================================
\subsection{Overfitting Revisited}
%=================================================

Recall the data model:
\[
y = f(x) + \varepsilon
\]

A neural network learns an approximation $\hat{f}(x)$.

Overfitting occurs when:
\begin{itemize}
    \item Training error is very low
    \item Test error is high
\end{itemize}

\subsubsection*{Strang Bridge — What Overfitting Really Is}

Overfitting is not complexity.
It is sensitivity to noise.

%=================================================
\subsection{Bias--Variance in Neural Networks}
%=================================================

The expected prediction error decomposes as:
\[
\mathbb{E}[(y - \hat{f}(x))^2]
=
\text{Bias}^2
+
\text{Variance}
+
\text{Noise}
\]

Neural networks:
\begin{itemize}
    \item Low bias
    \item Potentially high variance
\end{itemize}

Regularization controls variance.

%=================================================
\subsection{Explicit Regularization: L2 Weight Decay}
%=================================================

Modify the loss:
\[
\mathcal{L}_{\text{reg}}(\theta)
=
\mathcal{L}(\theta)
+
\lambda \sum_\ell \|W_\ell\|_F^2
\]

where:
\begin{itemize}
    \item $\lambda > 0$ controls regularization
    \item $\|\cdot\|_F$ is Frobenius norm
\end{itemize}

%-
\subsubsection*{Effect on Optimization}

Gradient becomes:
\[
\nabla \mathcal{L}_{\text{reg}}
=
\nabla \mathcal{L}
+
2\lambda W
\]

This shrinks weights toward zero.

\subsubsection*{Geometric Interpretation}

Regularization:
\begin{itemize}
    \item Penalizes large weights
    \item Favors smoother functions
\end{itemize}

Same principle as ridge regression.

%=================================================
\subsection{Implicit Regularization}
%=================================================

Even without explicit penalties,
neural networks regularize implicitly.

Sources include:
\begin{itemize}
    \item SGD noise
    \item Early stopping
    \item Architecture constraints
\end{itemize}

%=================================================
\subsection{Early Stopping}
%=================================================

Training error decreases monotonically.
Validation error typically:
\begin{itemize}
    \item Decreases initially
    \item Then increases
\end{itemize}

Early stopping halts training
before overfitting begins.

\subsubsection*{Strang Bridge — Why This Works}

Early stopping limits effective model complexity.

It behaves like L2 regularization.

%=================================================
\subsection{Dropout}
%=================================================

Dropout randomly removes neurons during training.

For each neuron:
\[
z \leftarrow
\begin{cases}
0 & \text{with probability } p \\
z & \text{with probability } 1-p
\end{cases}
\]

%-
\subsubsection*{Expectation View}

At test time, scaling ensures:
\[
\mathbb{E}[z_{\text{dropout}}] = z
\]

Dropout approximates:
\begin{itemize}
    \item Training an ensemble of networks
    \item Averaging their predictions
\end{itemize}

%=================================================
\subsection{Why Dropout Helps}
%=================================================

Dropout:
\begin{itemize}
    \item Prevents co-adaptation
    \item Forces robust feature learning
\end{itemize}

Geometrically:
\begin{itemize}
    \item Encourages distributed representations
\end{itemize}

%=================================================
\subsection{Data Augmentation}
%=================================================

Instead of constraining the model,
we expand the dataset.

Examples:
\begin{itemize}
    \item Rotations
    \item Translations
    \item Noise injection
\end{itemize}

Data augmentation encodes invariances.

%=================================================
\subsection{Capacity Control Beyond Parameters}
%=================================================

Generalization depends on:
\begin{itemize}
    \item Architecture
    \item Optimization dynamics
    \item Data structure
\end{itemize}

Parameter count alone does not determine overfitting.

%=================================================
\subsection{Modern View: Double Descent (Idea)}
%=================================================

As model size increases:
\begin{itemize}
    \item Error decreases
    \item Then increases
    \item Then decreases again
\end{itemize}

This challenges classical bias–variance intuition,
but does not invalidate it.

%=================================================
\subsection{What Regularization Cannot Do}
%=================================================

Regularization cannot:
\begin{itemize}
    \item Fix wrong labels
    \item Create missing information
    \item Replace data
\end{itemize}

Data quality remains fundamental.

%=================================================
\subsection{Summary of Module NN--6}
%=================================================

\begin{itemize}
    \item Overfitting is variance, not complexity
    \item L2 regularization shrinks weights
    \item SGD and early stopping regularize implicitly
    \item Dropout approximates model averaging
    \item Generalization is multi-factorial
\end{itemize}

This completes the theoretical core.

%=================================================
\section{NN--Module 7: Activation Functions}
%=================================================

Activation functions introduce nonlinearity into neural networks.
However, their role is deeper than mere nonlinearity.

They control:
\begin{itemize}
    \item Geometry of representations
    \item Gradient flow during training
    \item Stability of optimization
\end{itemize}

A poor activation choice can make learning impossible,
even if the network is theoretically expressive.

%=================================================
\subsection{What an Activation Function Does}
%=================================================

Each neuron computes:
\[
z = w^T x + b
\]
\[
a = \sigma(z)
\]

Thus, $\sigma$ transforms a linear response into a nonlinear signal.

\subsubsection*{Two Roles of $\sigma$}

\begin{enumerate}
    \item \textbf{Expressivity}: enables nonlinear functions
    \item \textbf{Trainability}: determines gradient behavior
\end{enumerate}

Both roles are equally important.

%=================================================
\subsection{Desirable Properties of Activations}
%=================================================

An activation function should ideally:
\begin{itemize}
    \item Be nonlinear
    \item Be differentiable (almost everywhere)
    \item Preserve gradient magnitude
    \item Be computationally simple
\end{itemize}

No activation satisfies all properties perfectly.
Trade-offs are unavoidable.

%=================================================
\subsection{Sigmoid Activation}
%=================================================

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

\subsubsection*{Geometry}

\begin{itemize}
    \item Compresses $\mathbb{R}$ into $(0,1)$
    \item Saturates for large $|z|$
\end{itemize}

\subsubsection*{Derivative}

\[
\sigma'(z) = \sigma(z)(1-\sigma(z))
\]

Maximum derivative:
\[
\max \sigma'(z) = \frac{1}{4}
\]

\subsubsection*{Consequence}

Repeated multiplication of numbers $<1$ leads to:
\[
\text{vanishing gradients}
\]

This makes deep sigmoid networks hard to train.

%=================================================
\subsection{Tanh Activation}
%=================================================

\[
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]

\subsubsection*{Properties}

\begin{itemize}
    \item Output range: $(-1,1)$
    \item Zero-centered
\end{itemize}

\subsubsection*{Derivative}

\[
\tanh'(z) = 1 - \tanh^2(z)
\]

Still saturates for large $|z|$.

\subsubsection*{Comparison with Sigmoid}

\begin{itemize}
    \item Better gradient near zero
    \item Same vanishing gradient issue at extremes
\end{itemize}

%=================================================
\subsection{ReLU (Rectified Linear Unit)}
%=================================================

\[
\sigma(z) = \max(0,z)
\]

\subsubsection*{Geometry}

\begin{itemize}
    \item Identity map for $z>0$
    \item Zero for $z<0$
\end{itemize}

This creates piecewise-linear functions.

%-
\subsubsection*{Derivative}

\[
\sigma'(z) =
\begin{cases}
1, & z>0 \\
0, & z<0
\end{cases}
\]

(ReLU is non-differentiable at $z=0$, but this is not problematic.)

%-
\subsubsection*{Why ReLU Changed Everything}

\begin{itemize}
    \item No saturation for positive inputs
    \item Gradients do not shrink
    \item Enables very deep networks
\end{itemize}

This solved the vanishing gradient crisis.

%=================================================
\subsection{ReLU Failure Mode: Dying ReLU}
%=================================================

If a neuron receives:
\[
z < 0 \quad \text{always}
\]

Then:
\[
\sigma(z) = 0, \quad \sigma'(z) = 0
\]

The neuron stops learning.

\subsubsection*{Causes}

\begin{itemize}
    \item Large negative bias
    \item Too large learning rate
\end{itemize}

%=================================================
\subsection{Leaky ReLU and Variants}
%=================================================

Leaky ReLU:
\[
\sigma(z) =
\begin{cases}
z, & z>0 \\
\alpha z, & z<0
\end{cases}
\quad (\alpha \ll 1)
\]

Purpose:
\begin{itemize}
    \item Allow small gradients for $z<0$
    \item Avoid dying neurons
\end{itemize}

Variants include:
\begin{itemize}
    \item Parametric ReLU
    \item ELU
\end{itemize}

%=================================================
\subsection{Softmax (Output Activation)}
%=================================================

For multi-class classification:
\[
\text{softmax}(z)_k = \frac{e^{z_k}}{\sum_j e^{z_j}}
\]

\subsubsection*{Properties}

\begin{itemize}
    \item Outputs form a probability distribution
    \item Encourages competition between classes
\end{itemize}

\subsubsection*{Gradient Interaction}

Softmax couples outputs:
\begin{itemize}
    \item Changing one logit affects all probabilities
\end{itemize}

This pairs naturally with cross-entropy loss.

%=================================================
\subsection{Activation Functions and Backpropagation}
%=================================================

Backpropagation multiplies derivatives:
\[
\delta_\ell =
(W_{\ell+1}^T \delta_{\ell+1})
\odot
\sigma'(a_\ell)
\]

Thus:
\begin{itemize}
    \item Small $\sigma'(a_\ell)$ kills gradients
    \item Constant $\sigma'(a_\ell)$ preserves gradients
\end{itemize}

Activation choice directly controls gradient flow.

%=================================================
\subsection{Choosing Activations in Practice}
%=================================================

General guidelines:
\begin{itemize}
    \item Hidden layers: ReLU or variants
    \item Output layer:
    \begin{itemize}
        \item Regression: linear
        \item Binary classification: sigmoid
        \item Multi-class: softmax
    \end{itemize}
\end{itemize}

These choices follow mathematical necessity, not convention.

%=================================================
\subsection{What Activations Cannot Fix}
%=================================================

Activations cannot:
\begin{itemize}
    \item Fix bad data
    \item Replace regularization
    \item Eliminate need for optimization tuning
\end{itemize}

They are necessary, but not sufficient.

%=================================================
\subsection{Summary of Module NN--7}
%=================================================

\begin{itemize}
    \item Activations control expressivity and trainability
    \item Sigmoid and tanh suffer from saturation
    \item ReLU preserves gradients
    \item Dying ReLU is a real failure mode
    \item Activation choice is mathematically consequential
\end{itemize}
%=================================================
\section{NN--Module 8: Neural Networks — Exam View and Mental Map}
%=================================================

This module consolidates everything learned so far
into a clear, exam-safe and theory-safe understanding.

Neural Networks are not about memorizing architectures.
They are about understanding a small number of core principles deeply.

%=================================================
\subsection{What Neural Networks Really Are}
%=================================================

A neural network is:

\begin{quote}
A parametric function formed by composing affine maps
and nonlinearities, trained by gradient-based optimization.
\end{quote}

Nothing more.
Nothing less.

Every neural network component serves exactly one purpose.

%=================================================
\subsection{Core Building Blocks (Non-Negotiable)}
%=================================================

Every neural network consists of:

\begin{enumerate}
    \item Linear algebra
    \[
    z = Wx + b
    \]
    \item Nonlinearity
    \[
    a = \sigma(z)
    \]
    \item Loss function
    \[
    L(y,\hat{y})
    \]
    \item Optimization
    \[
    \theta \leftarrow \theta - \eta \nabla L
    \]
\end{enumerate}

If any one block is weak, the network fails.

%=================================================
\subsection{What GATE Actually Tests (Neural Networks)}
%=================================================

GATE does \textbf{not} test:
\begin{itemize}
    \item CNN architectures
    \item Transformers
    \item Training tricks
\end{itemize}

GATE \textbf{does} test:
\begin{itemize}
    \item Perceptron geometry
    \item Linear separability
    \item Backpropagation logic
    \item Loss–activation pairing
    \item Vanishing gradients
\end{itemize}

Conceptual clarity beats formula memorization.

%=================================================
\subsection{High-Yield Exam Facts}
%=================================================

\begin{itemize}
    \item Without nonlinearity, depth is useless
    \item XOR is not linearly separable
    \item Backpropagation is chain rule, not learning
    \item SGD noise helps optimization
    \item ReLU alleviates vanishing gradients
\end{itemize}

These appear repeatedly in exam questions.

%=================================================
\subsection{Common Traps (Very Important)}
%=================================================

\begin{itemize}
    \item Confusing backpropagation with gradient descent
    \item Assuming more layers always improve performance
    \item Thinking neural networks require probability models
    \item Believing overfitting is solved by architecture alone
\end{itemize}

Most wrong answers come from these confusions.

%=================================================
\subsection{Neural Networks vs Classical ML}
%=================================================

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Aspect & Classical ML & Neural Networks \\
\hline
Feature design & Manual & Learned \\
Optimization & Often convex & Non-convex \\
Expressivity & Limited & Very high \\
Interpretability & Higher & Lower \\
\hline
\end{tabular}
\end{center}

Neural networks trade interpretability for flexibility.

%=================================================
\subsection{When Neural Networks Fail}
%=================================================

Neural networks fail when:
\begin{itemize}
    \item Data is insufficient
    \item Learning rate is poorly chosen
    \item Gradients vanish or explode
    \item Problem is fundamentally linear
\end{itemize}

Using NN blindly is a mistake.

%=================================================
\subsection{One-Page Mental Map (Core Insight)}
%=================================================

\begin{quote}
\textbf{Neural Network = Geometry + Calculus + Optimization}
\end{quote}

\begin{itemize}
    \item Geometry defines representation
    \item Calculus defines learning
    \item Optimization defines success
\end{itemize}

If training fails, exactly one of these is broken.

%=================================================
\subsection{How to Debug a Neural Network (Theory View)}
%=================================================

Always ask:
\begin{enumerate}
    \item Is the function expressive enough?
    \item Are gradients flowing?
    \item Is optimization stable?
    \item Is generalization controlled?
\end{enumerate}

This checklist solves most NN issues.

%=================================================
\subsection{Relationship to Everything Else You Learned}
%=================================================

Neural networks generalize:
\begin{itemize}
    \item Linear regression
    \item Logistic regression
    \item Basis expansions
\end{itemize}

They do not replace mathematics.
They \textbf{use} mathematics aggressively.

%=================================================
\subsection{Final Summary of Neural Networks}
%=================================================

\begin{itemize}
    \item Neural networks exist because linear models fail
    \item Neurons are geometric objects
    \item Depth is composition, not magic
    \item Backpropagation is chain rule
    \item Optimization and regularization determine success
\end{itemize}

\end{document}