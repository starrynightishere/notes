\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}

\geometry{margin=1in}

\title{\textbf{Neural Networks Practice Book}\\
(Geometry $\rightarrow$ Probability $\rightarrow$ Optimization)}
\author{}
\date{}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{How to Use This Book}
%=================================================

This is a \textbf{practice-first book}.

Rules:
\begin{itemize}
    \item Always attempt problems before looking at answers
    \item Refer back to the theory notes whenever stuck
    \item Write full reasoning, not just formulas
\end{itemize}

The goal is not speed.
The goal is \textbf{fundamental strength}.

%=================================================
\section{Chapter 1: What Is a Neuron, Really?}
%=================================================

A neural network does not start with learning.
It starts with \textbf{geometry}.

Before gradients, loss functions, or training,
you must understand what a single neuron represents.

%=================================================
\subsection*{Warm-Up: Thinking Without Math}
%=================================================

\textbf{Problem 1.1}

A neuron takes inputs and produces an output.

In your own words:
\begin{enumerate}
    \item What kind of object is the input?
    \item What kind of object is the output?
    \item What transformation happens in between?
\end{enumerate}

Do not write formulas yet.

\vspace{3cm}

%-
\textbf{Problem 1.2}

Explain the difference between:
\begin{itemize}
    \item A rule-based system
    \item A neuron
\end{itemize}

Why is a neuron more flexible?

\vspace{3cm}

%=================================================
\subsection{A Neuron Without Activation (Pure Geometry)}
%=================================================

\textbf{Problem 1.3 (Guided Example)}

Consider a neuron without activation:
\[
z = w^T x + b
\]

\begin{enumerate}
    \item What geometric object is defined by $w^T x + b = 0$?
    \item What does the vector $w$ represent geometrically?
\end{enumerate}

\textbf{Guided Thinking (Strang Style):}

Forget neural networks.
Think only about linear algebra.

\vspace{4cm}

%=================================================
\subsection{Worked Example: Classifying Points}
%=================================================

\textbf{Problem 1.4 (Worked)}

Let:
\[
w = \begin{bmatrix}2 \\ -1\end{bmatrix},
\quad
b = -1
\]

\begin{enumerate}
    \item Write the equation of the decision boundary.
    \item Determine on which side of the boundary the point
    $x = (1,0)$ lies.
\end{enumerate}

\textbf{Solution Sketch (Read Carefully):}

\begin{itemize}
    \item The boundary is a line in $\mathbb{R}^2$
    \item The sign of $w^T x + b$ determines the side
\end{itemize}

This is classification \emph{before} learning.

\vspace{2cm}

%=================================================
\subsection{Role of the Bias}
%=================================================

\textbf{Problem 1.5}

Explain geometrically:
\begin{enumerate}
    \item What happens if $b = 0$?
    \item Why is bias essential?
\end{enumerate}

Draw a picture if needed.

\vspace{4cm}

%=================================================
\subsection{Activation Functions Enter}
%=================================================

\textbf{Problem 1.6}

A neuron outputs:
\[
a = \sigma(w^T x + b)
\]

Explain:
\begin{enumerate}
    \item What role $\sigma$ plays
    \item What role it does \emph{not} play
\end{enumerate}

Your answer must separate:
\begin{itemize}
    \item Geometry
    \item Output interpretation
\end{itemize}

\vspace{4cm}

%=================================================
\subsection{Worked Example: Step vs Sigmoid}
%=================================================

\textbf{Problem 1.7 (Guided Example)}

Suppose the same hyperplane is used with:
\begin{itemize}
    \item Step activation
    \item Sigmoid activation
\end{itemize}

\begin{enumerate}
    \item Does the decision boundary change?
    \item What changes in the output?
\end{enumerate}

\textbf{Guided Insight:}

Boundary comes from $w^T x + b = 0$,
not from $\sigma$.

\vspace{4cm}

%=================================================
\subsection{Scaling Parameters}
%=================================================

\textbf{Problem 1.8 (Very Important)}

Suppose we replace $(w,b)$ with $(\alpha w, \alpha b)$,
where $\alpha > 0$.

\begin{enumerate}
    \item Does the geometry change?
    \item Does the output of sigmoid change?
\end{enumerate}

Explain carefully.

\vspace{4cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 1.9}

Which statements are true?

\begin{enumerate}
    \item A neuron defines a hyperplane
    \item Activation function determines the boundary
    \item Bias shifts the boundary
    \item Scaling weights always changes classification
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Reflection (Foundation Check)}
%=================================================

\textbf{Problem 1.10}

Complete the sentence:

\begin{quote}
A single neuron is fundamentally a \underline{\hspace{5cm}}
with a nonlinear output.
\end{quote}

If this does not feel obvious,
pause and revisit your lifetime notes.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 1}
%=================================================

\textbf{Answer 1.1}

Input is a vector.
Output is a scalar.
Transformation is affine + nonlinearity.

%-
\textbf{Answer 1.2}

Rules are fixed.
Neurons learn parameters.

%-
\textbf{Answer 1.3}

A hyperplane.
$w$ is the normal vector.

%-
\textbf{Answer 1.4}

Boundary: $2x_1 - x_2 - 1 = 0$.
Point lies on positive side.

%-
\textbf{Answer 1.5}

Without bias, boundary passes through origin.
Bias allows translation.

%-
\textbf{Answer 1.6}

$\sigma$ maps scores to outputs.
It does not change geometry.

%-
\textbf{Answer 1.7}

Boundary unchanged.
Output interpretation changes.

%-
\textbf{Answer 1.8}

Geometry unchanged.
Sigmoid confidence increases.

%-
\textbf{Answer 1.9}

Correct: (1), (3)

%-
\textbf{Answer 1.10}

Hyperplane.

%=================================================
\section{Chapter 2: Perceptron Learning — How Geometry Moves}
%=================================================

In Chapter 1, a neuron was a static geometric object.

In this chapter, the geometry \emph{moves}.

Learning means adjusting the hyperplane
so that it aligns with the data.

%=================================================
\subsection*{Warm-Up: What Does Learning Mean Here?}
%=================================================

\textbf{Problem 2.1}

Suppose a neuron misclassifies a data point.

\begin{enumerate}
    \item What does this mean geometrically?
    \item What should learning try to change?
\end{enumerate}

Answer without formulas.

\vspace{3cm}

%=================================================
\subsection{The Perceptron Model}
%=================================================

\textbf{Problem 2.2}

The perceptron outputs:
\[
\hat{y} = \text{sign}(w^T x + b)
\]

\begin{enumerate}
    \item What values can $\hat{y}$ take?
    \item What kind of problems can this model solve?
\end{enumerate}

Explain conceptually.

\vspace{3cm}

%=================================================
\subsection{The Learning Rule (Guided)}
%=================================================

\textbf{Problem 2.3 (Guided Example)}

The perceptron update rule is:
\[
w \leftarrow w + \eta\, y_i x_i,
\quad
b \leftarrow b + \eta\, y_i
\]

applied only when a point $(x_i,y_i)$ is misclassified.

\begin{enumerate}
    \item Why update only on mistakes?
    \item What does adding $y_i x_i$ do geometrically?
\end{enumerate}

\textbf{Guided Thinking (Strang Style):}

Think in terms of dot products and angles.

\vspace{4cm}

%=================================================
\subsection{Worked Example: One Update Step}
%=================================================

\textbf{Problem 2.4 (Worked)}

Let:
\[
w = \begin{bmatrix}0 \\ 0\end{bmatrix},
\quad
b = 0,
\quad
\eta = 1
\]

A data point:
\[
x = \begin{bmatrix}1 \\ 1\end{bmatrix},
\quad
y = +1
\]

\begin{enumerate}
    \item Is the point classified correctly?
    \item Compute the updated $w$ and $b$.
\end{enumerate}

\textbf{Solution Sketch:}

Initial output is zero → mistake.  
Update moves $w$ toward $x$.

\vspace{3cm}

%=================================================
\subsection{Why the Update Makes Sense}
%=================================================

\textbf{Problem 2.5}

Explain geometrically:

\begin{enumerate}
    \item Why adding $x_i$ helps when $y_i=+1$
    \item Why subtracting $x_i$ helps when $y_i=-1$
\end{enumerate}

Your explanation must mention:
\begin{itemize}
    \item Angle between $w$ and $x_i$
    \item Sign of the dot product
\end{itemize}

\vspace{4cm}

%=================================================
\subsection{Linear Separability}
%=================================================

\textbf{Problem 2.6}

Define in words:

\begin{quote}
What does it mean for data to be linearly separable?
\end{quote}

Why is this assumption crucial for the perceptron?

\vspace{3cm}

%=================================================
\subsection{Perceptron Convergence (Conceptual)}
%=================================================

\textbf{Problem 2.7 (Very Important)}

The perceptron convergence theorem states:

\begin{quote}
If data is linearly separable, the perceptron will converge.
\end{quote}

Explain:
\begin{enumerate}
    \item What convergence means here
    \item What it does \emph{not} guarantee
\end{enumerate}

\vspace{4cm}

%=================================================
\subsection{Failure Case: XOR}
%=================================================

\textbf{Problem 2.8}

Consider the XOR dataset.

\begin{enumerate}
    \item Why is XOR not linearly separable?
    \item What does the perceptron do when trained on XOR?
\end{enumerate}

Explain geometrically.

\vspace{4cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 2.9}

Which statements are true?

\begin{enumerate}
    \item Perceptron minimizes a convex loss
    \item Perceptron updates only on misclassification
    \item Perceptron always finds the maximum margin separator
    \item Perceptron can solve XOR
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Reflection (Foundation Check)}
%=================================================

\textbf{Problem 2.10}

Complete the sentence:

\begin{quote}
Perceptron learning works by repeatedly \underline{\hspace{6cm}}
until mistakes disappear.
\end{quote}

If this does not feel geometrically clear,
revisit Chapter 1 before proceeding.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 2}
%=================================================

\textbf{Answer 2.1}

The point lies on the wrong side of the boundary.
The boundary must move.

%-
\textbf{Answer 2.2}

Outputs $\{-1,+1\}$.
Solves linearly separable classification.

%-
\textbf{Answer 2.3}

Only mistakes provide information.
Update rotates $w$ toward correct classification.

%-
\textbf{Answer 2.4}

Misclassified.
Updated:
\[
w = \begin{bmatrix}1 \\ 1\end{bmatrix}, \quad b = 1
\]

%-
\textbf{Answer 2.5}

Updates increase correct dot product sign.

%-
\textbf{Answer 2.6}

A hyperplane separates classes.
Without it, convergence is impossible.

%-
\textbf{Answer 2.7}

Convergence means zero training error.
It does not guarantee optimal margin.

%-
\textbf{Answer 2.8}

No single line separates XOR.
Perceptron cycles forever.

%-
\textbf{Answer 2.9}

Correct: (2)

%-
\textbf{Answer 2.10}

Moving the hyperplane.
%=================================================
\section{Chapter 3: Why One Layer Is Not Enough}
%=================================================

In Chapter 2, we saw that a single perceptron can learn
only linearly separable problems.

This chapter answers the fundamental question:

\begin{quote}
Why does adding layers change what can be learned?
\end{quote}

The answer lies in \textbf{representation}, not optimization.

%=================================================
\subsection*{Warm-Up: Revisiting a Failure}
%=================================================

\textbf{Problem 3.1}

Recall the XOR classification problem.

\begin{enumerate}
    \item How many classes are there?
    \item How many linear regions are needed to separate them?
\end{enumerate}

Explain using geometry, not equations.

\vspace{3cm}

%=================================================
\subsection{Why XOR Breaks a Single Hyperplane}
%=================================================

\textbf{Problem 3.2}

Consider four points in $\mathbb{R}^2$ forming XOR.

\begin{enumerate}
    \item Try to draw a single line that separates the classes.
    \item Explain why this is impossible.
\end{enumerate}

What geometric limitation is being exposed?

\vspace{4cm}

%=================================================
\subsection{Key Insight: Features vs Decisions}
%=================================================

\textbf{Problem 3.3 (Guided)}

A perceptron makes decisions in the \emph{input space}.

\begin{enumerate}
    \item What if the data were represented differently?
    \item Could the same classifier succeed?
\end{enumerate}

\textbf{Guided Thinking (Strang Style):}

Do not change the classifier.
Change the space.

\vspace{4cm}

%=================================================
\subsection{Hidden Layer as Feature Transformer}
%=================================================

\textbf{Problem 3.4}

A two-layer network computes:
\[
x \;\longrightarrow\; z = \sigma(W_1 x + b_1)
\;\longrightarrow\;
\hat{y} = \text{sign}(w_2^T z + b_2)
\]

Explain:
\begin{enumerate}
    \item What the first layer is doing
    \item Why the second layer is now more powerful
\end{enumerate}

Answer conceptually.

\vspace{4cm}

%=================================================
\subsection{Worked Example: XOR with Two Neurons}
%=================================================

\textbf{Problem 3.5 (Worked)}

Consider XOR inputs:
\[
(0,0), (1,1) \rightarrow -1,
\quad
(1,0), (0,1) \rightarrow +1
\]

Suppose the hidden layer learns:
\begin{itemize}
    \item Neuron 1: $x_1 + x_2 > 0.5$
    \item Neuron 2: $x_1 + x_2 < 1.5$
\end{itemize}

\begin{enumerate}
    \item What regions of space do these neurons activate?
    \item Why does this make XOR linearly separable in $z$-space?
\end{enumerate}

\textbf{Guided Insight:}

XOR is solved by slicing space, not bending lines.

\vspace{4cm}

%=================================================
\subsection{Composition of Functions}
%=================================================

\textbf{Problem 3.6}

A deep network computes:
\[
f(x) = f_3(f_2(f_1(x)))
\]

Explain:
\begin{enumerate}
    \item Why composition increases expressivity
    \item Why depth is different from width
\end{enumerate}

Use examples, not formulas.

\vspace{4cm}

%=================================================
\subsection{Geometry of Depth}
%=================================================

\textbf{Problem 3.7}

Explain geometrically how:
\begin{itemize}
    \item One layer draws a hyperplane
    \item Two layers carve regions
    \item Many layers fold space
\end{itemize}

Relate this to real-world data.

\vspace{4cm}

%=================================================
\subsection{Depth vs Linear Models}
%=================================================

\textbf{Problem 3.8}

Explain why stacking linear layers
\emph{without activations}
does not increase expressivity.

What exactly fails?

\vspace{3cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 3.9}

Which statements are true?

\begin{enumerate}
    \item A two-layer network can solve XOR
    \item Depth matters only for optimization
    \item Hidden layers learn representations
    \item Linear activations suffice for deep learning
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Reflection (Core Insight)}
%=================================================

\textbf{Problem 3.10}

Complete the sentence:

\begin{quote}
Neural networks work because they \underline{\hspace{6cm}},
not because they memorize.
\end{quote}

If your answer does not mention representation,
pause and revisit this chapter.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 3}
%=================================================

\textbf{Answer 3.1}

Two classes.
Requires multiple regions.

%-
\textbf{Answer 3.2}

No single line separates the classes.
Linear decision is insufficient.

%-
\textbf{Answer 3.3}

A better representation can make separation possible.

%-
\textbf{Answer 3.4}

First layer transforms features.
Second layer separates them.

%-
\textbf{Answer 3.5}

Hidden neurons carve regions.
XOR becomes separable.

%-
\textbf{Answer 3.6}

Composition builds complex functions from simple ones.
Depth reuses structure.

%-
\textbf{Answer 3.7}

Layers progressively fold space.

%-
\textbf{Answer 3.8}

Linear maps compose to a linear map.

%-
\textbf{Answer 3.9}

Correct: (1), (3)

%-
\textbf{Answer 3.10}

Learn representations.

%=================================================
\section{Chapter 4: Backpropagation — How Learning Flows}
%=================================================

In previous chapters:
\begin{itemize}
    \item A neuron defined geometry
    \item Learning moved geometry
    \item Depth changed representation
\end{itemize}

Now we answer the central question:

\begin{quote}
How do we compute updates when there are many layers?
\end{quote}

The answer is: \textbf{the chain rule}.

%=================================================
\subsection*{Warm-Up: Sensitivity Thinking}
%=================================================

\textbf{Problem 4.1}

Suppose changing a weight slightly
changes the final output slightly.

\begin{enumerate}
    \item What does this mean geometrically?
    \item What quantity measures this sensitivity?
\end{enumerate}

Answer without formulas.

\vspace{3cm}

%=================================================
\subsection{Loss Function First}
%=================================================

\textbf{Problem 4.2}

A neural network produces an output $\hat{y}$.
We define a loss:
\[
\mathcal{L}(\hat{y}, y)
\]

\begin{enumerate}
    \item Why do we need a loss function?
    \item What does minimizing loss mean geometrically?
\end{enumerate}

Explain intuitively.

\vspace{3cm}

%=================================================
\subsection{Single Neuron Gradient (Guided)}
%=================================================

\textbf{Problem 4.3 (Guided Example)}

Consider:
\[
z = w^T x + b,
\quad
a = \sigma(z),
\quad
\mathcal{L} = \mathcal{L}(a)
\]

\begin{enumerate}
    \item List all dependencies from $w$ to $\mathcal{L}$
    \item In what order does information flow forward?
\end{enumerate}

\textbf{Guided Thinking (Strang Style):}

Draw a computation graph.

\vspace{4cm}

%=================================================
\subsection{Chain Rule as Information Flow}
%=================================================

\textbf{Problem 4.4}

Explain in words:

\begin{quote}
Why does the gradient flow backward
while computation flows forward?
\end{quote}

Your explanation must reference:
\begin{itemize}
    \item Cause and effect
    \item Local sensitivity
\end{itemize}

\vspace{4cm}

%=================================================
\subsection{Worked Example: Scalar Chain Rule}
%=================================================

\textbf{Problem 4.5 (Worked)}

Let:
\[
f(x) = (3x + 1)^2
\]

\begin{enumerate}
    \item Identify intermediate variables
    \item Compute $\frac{df}{dx}$ step by step
\end{enumerate}

\textbf{Solution Sketch:}

Let $u = 3x + 1$, then $f = u^2$.  
Apply chain rule.

\vspace{3cm}

%=================================================
\subsection{Backprop in a Two-Layer Network}
%=================================================

\textbf{Problem 4.6}

Consider:
\[
x \rightarrow z^{(1)} \rightarrow a^{(1)}
\rightarrow z^{(2)} \rightarrow a^{(2)} = \hat{y}
\]

\begin{enumerate}
    \item Which quantities depend on layer 1 weights?
    \item Why can gradients be reused efficiently?
\end{enumerate}

Answer conceptually.

\vspace{4cm}

%=================================================
\subsection{Local Gradients}
%=================================================

\textbf{Problem 4.7}

Explain why backprop relies on:
\begin{quote}
Local derivatives multiplied together
\end{quote}

What advantage does this give computationally?

\vspace{3cm}

%=================================================
\subsection{Worked Example: One Backprop Step}
%=================================================

\textbf{Problem 4.8 (Guided)}

Assume:
\[
\mathcal{L} = \frac{1}{2}(a - y)^2,
\quad
a = \sigma(z),
\quad
z = wx
\]

\begin{enumerate}
    \item Compute $\frac{\partial \mathcal{L}}{\partial a}$
    \item Compute $\frac{\partial a}{\partial z}$
    \item Combine to get $\frac{\partial \mathcal{L}}{\partial w}$
\end{enumerate}

Focus on structure, not algebra.

\vspace{4cm}

%=================================================
\subsection{Why Backprop Is Efficient}
%=================================================

\textbf{Problem 4.9}

Explain why naive differentiation
would be exponentially expensive,
but backprop is linear in number of parameters.

Use the idea of reuse.

\vspace{4cm}

%=================================================
\subsection{Common Misconceptions}
%=================================================

\textbf{Problem 4.10}

Explain why backprop:
\begin{itemize}
    \item Is not a learning algorithm
    \item Does not depend on sigmoid specifically
\end{itemize}

What role does it actually play?

\vspace{4cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 4.11}

Which statements are true?

\begin{enumerate}
    \item Backprop is an application of chain rule
    \item Gradients flow in the same direction as activations
    \item Backprop computes exact gradients
    \item Backprop requires convex loss
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Reflection (Deep Check)}
%=================================================

\textbf{Problem 4.12}

Complete the sentence:

\begin{quote}
Backpropagation works because complex derivatives can be
computed by \underline{\hspace{6cm}}.
\end{quote}

If this feels vague,
pause and revisit the computation graph idea.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 4}
%=================================================

\textbf{Answer 4.1}

Output sensitivity.
Measured by gradients.

%-
\textbf{Answer 4.2}

Loss defines objective.
Minimization aligns predictions with targets.

%-
\textbf{Answer 4.3}

$w \rightarrow z \rightarrow a \rightarrow \mathcal{L}$.
Forward flow follows this order.

%-
\textbf{Answer 4.4}

Effect depends on causes.
Gradients propagate responsibility backward.

%-
\textbf{Answer 4.5}

$\frac{df}{dx} = 2(3x+1)\cdot 3$

%-
\textbf{Answer 4.6}

Layer 1 affects all downstream outputs.
Gradients are reused.

%-
\textbf{Answer 4.7}

Local gradients compose.
Enables dynamic programming.

%-
\textbf{Answer 4.8}

Chain rule combines sensitivities.

%-
\textbf{Answer 4.9}

Backprop avoids repeated computation.

%-
\textbf{Answer 4.10}

Backprop computes gradients only.

%-
\textbf{Answer 4.11}

Correct: (1), (3)

%-
\textbf{Answer 4.12}

Repeated application of the chain rule.

%=================================================
\section{Chapter 5: Activation Functions and Gradient Pathologies}
%=================================================

Backpropagation tells us \emph{how} to compute gradients.
Activation functions determine \emph{whether those gradients are useful}.

This chapter studies why learning can:
\begin{itemize}
    \item slow down
    \item stop
    \item or explode
\end{itemize}

%=================================================
\subsection*{Warm-Up: Why Not Linear Everywhere?}
%=================================================

\textbf{Problem 5.1}

Suppose all activations are linear.

\begin{enumerate}
    \item What does a deep network reduce to?
    \item Why is this a problem for expressivity?
\end{enumerate}

Explain without formulas.

\vspace{3cm}

%=================================================
\subsection{Sigmoid Activation}
%=================================================

\textbf{Problem 5.2}

The sigmoid function is:
\[
\sigma(z) = \frac{1}{1+e^{-z}}
\]

\begin{enumerate}
    \item What range does $\sigma(z)$ map to?
    \item Why was sigmoid historically appealing?
\end{enumerate}

Answer conceptually.

\vspace{3cm}

%=================================================
\subsection{Sigmoid Geometry}
%=================================================

\textbf{Problem 5.3 (Guided)}

Sketch (mentally or on paper) the sigmoid curve.

\begin{enumerate}
    \item Where is it steep?
    \item Where is it flat?
\end{enumerate}

Explain what “flat” means for learning.

\vspace{4cm}

%=================================================
\subsection{Vanishing Gradients}
%=================================================

\textbf{Problem 5.4 (Very Important)}

Backprop multiplies derivatives across layers.

\begin{enumerate}
    \item What happens if each derivative is $<1$?
    \item What happens as depth increases?
\end{enumerate}

Explain why this causes vanishing gradients.

\vspace{4cm}

%=================================================
\subsection{Worked Example: Chain of Sigmoids}
%=================================================

\textbf{Problem 5.5 (Guided Example)}

Suppose:
\[
\frac{d\sigma}{dz} \le 0.25
\]

for all $z$.

\begin{enumerate}
    \item What happens to gradients after 10 layers?
    \item What happens after 100 layers?
\end{enumerate}

No calculation needed — reason qualitatively.

\vspace{4cm}

%=================================================
\subsection{Exploding Gradients}
%=================================================

\textbf{Problem 5.6}

Now suppose weights are large.

\begin{enumerate}
    \item What happens to gradients during backprop?
    \item Why does optimization become unstable?
\end{enumerate}

Explain in terms of multiplication.

\vspace{3cm}

%=================================================
\subsection{Tanh Activation}
%=================================================

\textbf{Problem 5.7}

The tanh function maps:
\[
\mathbb{R} \rightarrow (-1,1)
\]

\begin{enumerate}
    \item How is tanh related to sigmoid?
    \item Why is tanh often preferred over sigmoid?
\end{enumerate}

Answer geometrically and statistically.

\vspace{4cm}

%=================================================
\subsection{ReLU Activation (The Breakthrough)}
%=================================================

\textbf{Problem 5.8}

ReLU is defined as:
\[
\text{ReLU}(z) = \max(0,z)
\]

\begin{enumerate}
    \item What is the derivative when $z>0$?
    \item What is the derivative when $z<0$?
\end{enumerate}

Why is this good for gradient flow?

\vspace{4cm}

%=================================================
\subsection{Geometry of ReLU}
%=================================================

\textbf{Problem 5.9}

Explain geometrically how ReLU:
\begin{itemize}
    \item Preserves linearity locally
    \item Introduces nonlinearity globally
\end{itemize}

Relate this to piecewise linear functions.

\vspace{4cm}

%=================================================
\subsection{Dead Neurons}
%=================================================

\textbf{Problem 5.10}

A ReLU neuron can become “dead”.

\begin{enumerate}
    \item What does this mean?
    \item How can initialization or learning rate cause this?
\end{enumerate}

Explain carefully.

\vspace{4cm}

%=================================================
\subsection{Choosing Activations}
%=================================================

\textbf{Problem 5.11}

Match the activation to the situation:

\begin{enumerate}
    \item Binary classification output
    \item Hidden layers in deep networks
    \item Output with real-valued regression
\end{enumerate}

Justify each choice.

\vspace{4cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 5.12}

Which statements are true?

\begin{enumerate}
    \item Sigmoid suffers from vanishing gradients
    \item ReLU completely eliminates gradient problems
    \item Tanh is zero-centered
    \item Exploding gradients are impossible with ReLU
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Reflection (Core Insight)}
%=================================================

\textbf{Problem 5.13}

Complete the sentence:

\begin{quote}
Activation functions matter because they control
\underline{\hspace{6cm}}.
\end{quote}

If this answer is unclear,
revisit the chain rule and this chapter together.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 5}
%=================================================

\textbf{Answer 5.1}

A linear map.
No added expressivity.

%-
\textbf{Answer 5.2}

Maps to $(0,1)$.
Interpretable as probability.

%-
\textbf{Answer 5.3}

Flat regions imply near-zero gradients.

%-
\textbf{Answer 5.4}

Gradients shrink exponentially.

%-
\textbf{Answer 5.5}

They vanish rapidly.

%-
\textbf{Answer 5.6}

Gradients blow up.
Updates become unstable.

%-
\textbf{Answer 5.7}

Tanh is zero-centered.
Improves optimization.

%-
\textbf{Answer 5.8}

Derivative is $1$ for $z>0$.
Prevents shrinkage.

%-
\textbf{Answer 5.9}

Piecewise linear regions.

%-
\textbf{Answer 5.10}

Neuron outputs zero forever.

%-
\textbf{Answer 5.11}

Sigmoid.
ReLU.
Linear.

%-
\textbf{Answer 5.12}

Correct: (1), (3)

%-
\textbf{Answer 5.13}

Gradient flow.
%=================================================
\section{Chapter 6: Gradient Descent — How Networks Actually Learn}
%=================================================

Backpropagation computes gradients.
Gradient descent decides what to do with them.

This chapter studies learning as \textbf{motion on a surface}.

%=================================================
\subsection*{Warm-Up: Learning as Movement}
%=================================================

\textbf{Problem 6.1}

Imagine the loss as a surface over parameter space.

\begin{enumerate}
    \item What does a point on this surface represent?
    \item What does moving downhill mean?
\end{enumerate}

Explain without equations.

\vspace{3cm}

%=================================================
\subsection{The Gradient Vector}
%=================================================

\textbf{Problem 6.2}

The gradient of a function points in the direction of
steepest increase.

\begin{enumerate}
    \item What direction should we move to minimize loss?
    \item Why is the gradient a local quantity?
\end{enumerate}

Answer geometrically.

\vspace{3cm}

%=================================================
\subsection{Gradient Descent Update Rule}
%=================================================

\textbf{Problem 6.3 (Guided)}

Gradient descent updates parameters by:
\[
\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}
\]

\begin{enumerate}
    \item What does $\eta$ control?
    \item What happens if $\eta$ is too large?
    \item What happens if $\eta$ is too small?
\end{enumerate}

Think in terms of steps on terrain.

\vspace{4cm}

%=================================================
\subsection{Worked Example: One-Dimensional Loss}
%=================================================

\textbf{Problem 6.4 (Worked)}

Let:
\[
\mathcal{L}(w) = (w-2)^2
\]

\begin{enumerate}
    \item Where is the minimum?
    \item Starting from $w=0$, what direction is the update?
\end{enumerate}

\textbf{Solution Sketch:}

The gradient points away from the minimum.

\vspace{3cm}

%=================================================
\subsection{Batch vs Stochastic Gradient Descent}
%=================================================

\textbf{Problem 6.5}

Explain the difference between:
\begin{itemize}
    \item Batch gradient descent
    \item Stochastic gradient descent (SGD)
\end{itemize}

Which one introduces noise?
Why might that be beneficial?

\vspace{4cm}

%=================================================
\subsection{Why SGD Works Surprisingly Well}
%=================================================

\textbf{Problem 6.6 (Very Important)}

Deep networks have non-convex loss surfaces.

\begin{enumerate}
    \item Why doesn’t SGD get stuck easily?
    \item How does noise help exploration?
\end{enumerate}

Answer conceptually.

\vspace{4cm}

%=================================================
\subsection{Local Minima vs Saddle Points}
%=================================================

\textbf{Problem 6.7}

Explain the difference between:
\begin{itemize}
    \item Local minima
    \item Saddle points
\end{itemize}

Why are saddle points more problematic in high dimensions?

\vspace{4cm}

%=================================================
\subsection{Learning Rate Schedules}
%=================================================

\textbf{Problem 6.8}

Why might we:
\begin{itemize}
    \item Start with a large learning rate
    \item Gradually decrease it?
\end{itemize}

Explain in terms of exploration and convergence.

\vspace{4cm}

%=================================================
\subsection{Momentum Intuition}
%=================================================

\textbf{Problem 6.9}

Momentum modifies gradient descent.

\begin{enumerate}
    \item What physical analogy explains momentum?
    \item How does it help in ravines?
\end{enumerate}

Explain geometrically.

\vspace{4cm}

%=================================================
\subsection{Optimization vs Generalization}
%=================================================

\textbf{Problem 6.10}

Explain why:
\begin{quote}
Better optimization does not always mean better generalization.
\end{quote}

Relate this to:
\begin{itemize}
    \item Sharp minima
    \item Flat minima
\end{itemize}

\vspace{4cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 6.11}

Which statements are true?

\begin{enumerate}
    \item Gradient descent guarantees global minimum
    \item SGD introduces noise into updates
    \item Saddle points are rare in deep networks
    \item Learning rate affects convergence speed
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Reflection (Core Insight)}
%=================================================

\textbf{Problem 6.12}

Complete the sentence:

\begin{quote}
Training a neural network is equivalent to
\underline{\hspace{6cm}} on a high-dimensional surface.
\end{quote}

If this is unclear,
pause and mentally visualize loss landscapes.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 6}
%=================================================

\textbf{Answer 6.1}

A model configuration.
Moving reduces error.

%-
\textbf{Answer 6.2}

Move opposite to gradient.
Gradient is local.

%-
\textbf{Answer 6.3}

$\eta$ controls step size.
Too large: divergence.
Too small: slow learning.

%-
\textbf{Answer 6.4}

Minimum at $w=2$.
Move toward positive direction.

%-
\textbf{Answer 6.5}

SGD uses single samples.
Noise helps escape traps.

%-
\textbf{Answer 6.6}

Noise helps exploration.

%-
\textbf{Answer 6.7}

Saddle points have flat directions.
They dominate in high dimensions.

%-
\textbf{Answer 6.8}

Fast exploration first.
Fine convergence later.

%-
\textbf{Answer 6.9}

Rolling ball analogy.
Momentum smooths oscillations.

%-
\textbf{Answer 6.10}

Flat minima generalize better.

%-
\textbf{Answer 6.11}

Correct: (2), (4)

%-
\textbf{Answer 6.12}

Gradient-based optimization.
%=================================================
\section{Chapter 7: Initialization, Normalization, and Training Stability}
%=================================================

A neural network can fail before learning even begins.

This chapter answers:
\begin{quote}
Why do some networks train easily while others do not?
\end{quote}

The answer lies in how signals propagate through depth.

%=================================================
\subsection*{Warm-Up: Before Learning Starts}
%=================================================

\textbf{Problem 7.1}

Before any gradient descent step:
\begin{enumerate}
    \item What determines the initial outputs of a network?
    \item What determines the initial gradients?
\end{enumerate}

Explain without formulas.

\vspace{3cm}

%=================================================
\subsection{Why Initialization Matters}
%=================================================

\textbf{Problem 7.2}

Suppose all weights are initialized to zero.

\begin{enumerate}
    \item What happens to symmetry between neurons?
    \item Why does learning fail?
\end{enumerate}

Explain conceptually.

\vspace{4cm}

%=================================================
\subsection{Signal Propagation Through Layers}
%=================================================

\textbf{Problem 7.3}

Consider a deep network at initialization.

\begin{enumerate}
    \item What happens if weights are too small?
    \item What happens if weights are too large?
\end{enumerate}

Relate this to vanishing and exploding signals.

\vspace{4cm}

%=================================================
\subsection{Variance View (Key Insight)}
%=================================================

\textbf{Problem 7.4 (Very Important)}

Assume inputs have unit variance.

\begin{enumerate}
    \item How does variance change after one linear layer?
    \item What happens after many layers?
\end{enumerate}

Why is controlling variance crucial?

\vspace{4cm}

%=================================================
\subsection{Xavier and He Initialization}
%=================================================

\textbf{Problem 7.5}

Modern initialization schemes choose weight variance carefully.

\begin{enumerate}
    \item What is the goal of Xavier initialization?
    \item Why does He initialization work better with ReLU?
\end{enumerate}

Answer in terms of variance preservation.

\vspace{4cm}

%=================================================
\subsection{Normalization Idea}
%=================================================

\textbf{Problem 7.6}

Normalization rescales activations.

\begin{enumerate}
    \item What problem does normalization address?
    \item Why does normalization help gradient flow?
\end{enumerate}

Explain intuitively.

\vspace{4cm}

%=================================================
\subsection{Batch Normalization (Conceptual)}
%=================================================

\textbf{Problem 7.7}

Batch normalization normalizes layer inputs.

\begin{enumerate}
    \item What statistics are used?
    \item Why is this done during training?
\end{enumerate}

Do not write equations.

\vspace{4cm}

%=================================================
\subsection{Batch Norm as Optimization Aid}
%=================================================

\textbf{Problem 7.8}

Explain how batch normalization:
\begin{itemize}
    \item Smooths the loss landscape
    \item Allows larger learning rates
\end{itemize}

Relate this to stability.

\vspace{4cm}

%=================================================
\subsection{Regularization Side Effects}
%=================================================

\textbf{Problem 7.9}

Batch normalization often improves generalization.

\begin{enumerate}
    \item Why does it act like regularization?
    \item Why is this effect indirect?
\end{enumerate}

Explain carefully.

\vspace{4cm}

%=================================================
\subsection{Training Deep Networks: Big Picture}
%=================================================

\textbf{Problem 7.10}

List the components that must work together
for stable deep learning:

\begin{itemize}
    \item Initialization
    \item Activation
    \item Optimization
    \item Normalization
\end{itemize}

Explain the role of each in one sentence.

\vspace{5cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 7.11}

Which statements are true?

\begin{enumerate}
    \item Zero initialization breaks symmetry
    \item Poor initialization can prevent learning
    \item Batch normalization removes the need for activation functions
    \item Initialization affects gradient flow
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Reflection (Stability Check)}
%=================================================

\textbf{Problem 7.12}

Complete the sentence:

\begin{quote}
Deep networks train successfully only when
\underline{\hspace{6cm}} are kept under control.
\end{quote}

If this feels unclear,
revisit Chapters 4–7 together.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 7}
%=================================================

\textbf{Answer 7.1}

Initialization determines both.

%-
\textbf{Answer 7.2}

All neurons behave identically.
No symmetry breaking.

%-
\textbf{Answer 7.3}

Signals vanish or explode.

%-
\textbf{Answer 7.4}

Variance compounds across layers.

%-
\textbf{Answer 7.5}

Maintain variance.
Match activation behavior.

%-
\textbf{Answer 7.6}

Controls scale.
Stabilizes gradients.

%-
\textbf{Answer 7.7}

Mean and variance.
Reduces internal covariate shift.

%-
\textbf{Answer 7.8}

Smooths optimization.

%-
\textbf{Answer 7.9}

Noise from batch statistics.

%-
\textbf{Answer 7.10}

All control signal flow.

%-
\textbf{Answer 7.11}

Correct: (2), (4)

%-
\textbf{Answer 7.12}

Signal and gradient scales.
%=================================================
\section{Chapter 8: Generalization, Overfitting, and Regularization}
%=================================================

Neural networks often have:
\begin{itemize}
    \item More parameters than data points
    \item Zero training error
\end{itemize}

Yet they can still generalize.

This chapter explains why this is not magic.

%=================================================
\subsection*{Warm-Up: Classical Intuition}
%=================================================

\textbf{Problem 8.1}

In classical ML:
\begin{itemize}
    \item Simple models underfit
    \item Complex models overfit
\end{itemize}

Why would one expect neural networks to overfit badly?

Answer without mentioning deep learning yet.

\vspace{3cm}

%=================================================
\subsection{Bias--Variance in Neural Networks}
%=================================================

\textbf{Problem 8.2}

Explain how bias and variance appear in neural networks.

\begin{enumerate}
    \item What controls bias?
    \item What controls variance?
\end{enumerate}

Explain conceptually.

\vspace{4cm}

%=================================================
\subsection{Overparameterization Paradox}
%=================================================

\textbf{Problem 8.3 (Very Important)}

Modern neural networks are often overparameterized.

\begin{enumerate}
    \item What does overparameterized mean?
    \item Why doesn’t this automatically imply overfitting?
\end{enumerate}

Explain using optimization intuition.

\vspace{4cm}

%=================================================
\subsection{Implicit Regularization}
%=================================================

\textbf{Problem 8.4}

Even without explicit penalties,
neural networks often generalize well.

\begin{enumerate}
    \item How does SGD act as implicit regularization?
    \item What kind of solutions does it favor?
\end{enumerate}

Answer in terms of geometry of minima.

\vspace{4cm}

%=================================================
\subsection{Explicit Regularization}
%=================================================

\textbf{Problem 8.5}

Common explicit regularizers include:
\begin{itemize}
    \item Weight decay
    \item Dropout
\end{itemize}

Explain:
\begin{enumerate}
    \item What problem they target
    \item How they differ conceptually
\end{enumerate}

\vspace{4cm}

%=================================================
\subsection{Dropout Intuition}
%=================================================

\textbf{Problem 8.6}

Dropout randomly removes neurons during training.

\begin{enumerate}
    \item Why does this prevent co-adaptation?
    \item How is dropout similar to model averaging?
\end{enumerate}

Explain geometrically or statistically.

\vspace{4cm}

%=================================================
\subsection{Early Stopping Revisited}
%=================================================

\textbf{Problem 8.7}

Explain why stopping training early:
\begin{itemize}
    \item Increases bias slightly
    \item Reduces variance significantly
\end{itemize}

Why does this help generalization?

\vspace{4cm}

%=================================================
\subsection{Double Descent Phenomenon}
%=================================================

\textbf{Problem 8.8}

Modern learning curves sometimes show \emph{double descent}.

\begin{enumerate}
    \item What is classical U-shaped risk?
    \item What changes in the overparameterized regime?
\end{enumerate}

Explain qualitatively.

\vspace{4cm}

%=================================================
\subsection{Choosing Regularization}
%=================================================

\textbf{Problem 8.9}

Given a neural network that overfits:

Which of the following would you try first, and why?
\begin{itemize}
    \item More data
    \item Weight decay
    \item Dropout
    \item Early stopping
\end{itemize}

There is no single correct answer — justify your choice.

\vspace{5cm}

%=================================================
\subsection{Exam Traps (GATE Style)}
%=================================================

\textbf{Problem 8.10}

Which statements are true?

\begin{enumerate}
    \item More parameters always imply worse generalization
    \item SGD can act as regularization
    \item Dropout is used at test time
    \item Early stopping is a form of regularization
\end{enumerate}

(Choose all that apply.)

\vspace{3cm}

%=================================================
\subsection{Final Reflection (Big Picture)}
%=================================================

\textbf{Problem 8.11}

Complete the sentence:

\begin{quote}
Neural networks generalize well not because they are small,
but because \underline{\hspace{6cm}}.
\end{quote}

If this answer feels hand-wavy,
revisit Chapters 6–8 together.

\vspace{4cm}

%=================================================
\section{Answers — Chapter 8}
%=================================================

\textbf{Answer 8.1}

High complexity suggests overfitting.

%-
\textbf{Answer 8.2}

Architecture controls bias.
Data and regularization control variance.

%-
\textbf{Answer 8.3}

Many parameters, many solutions.
Optimization selects simple ones.

%-
\textbf{Answer 8.4}

SGD prefers flat minima.

%-
\textbf{Answer 8.5}

Reduce variance.
Different mechanisms.

%-
\textbf{Answer 8.6}

Forces redundancy.
Approximates ensemble.

%-
\textbf{Answer 8.7}

Prevents fitting noise.

%-
\textbf{Answer 8.8}

Risk decreases again after interpolation.

%-
\textbf{Answer 8.9}

Depends on failure mode.

%-
\textbf{Answer 8.10}

Correct: (2), (4)

%-
\textbf{Answer 8.11}

Optimization and regularization shape solutions.

%=================================================
\section{Chapter 9: Mixed Reasoning Problems}
%=================================================

This chapter tests whether neural network ideas
have fused into a single mental model.

Each problem mixes:
\begin{itemize}
    \item Geometry
    \item Calculus
    \item Optimization
    \item Generalization
\end{itemize}

You are expected to jump back to Chapters 1--8 when needed.

%=================================================
\subsection*{Problem Set A: Geometry + Depth}
%=================================================

\textbf{Problem 9.1}

Explain why adding a hidden layer can reduce
\emph{both} bias and variance in some cases.

Why does this not contradict the bias--variance trade-off?

\vspace{4cm}

%-
\textbf{Problem 9.2}

A deep network with ReLU activations
represents a piecewise linear function.

\begin{enumerate}
    \item What determines the number of linear regions?
    \item Why does depth increase this number exponentially?
\end{enumerate}

Explain geometrically.

\vspace{4cm}

%=================================================
\subsection*{Problem Set B: Backprop + Optimization}
%=================================================

\textbf{Problem 9.3}

Explain why backpropagation alone
does not guarantee learning.

What two additional components are required?

\vspace{3cm}

%-
\textbf{Problem 9.4}

Consider two networks:
\begin{itemize}
    \item Network A: shallow, wide
    \item Network B: deep, narrow
\end{itemize}

Both have the same number of parameters.

\begin{enumerate}
    \item Which is harder to optimize?
    \item Which is more expressive?
\end{enumerate}

Justify your answers conceptually.

\vspace{4cm}

%=================================================
\subsection*{Problem Set C: Gradient Pathologies}
%=================================================

\textbf{Problem 9.5}

Explain why vanishing gradients are primarily
a depth problem, not a data problem.

Why does adding more data not fix it?

\vspace{4cm}

%-
\textbf{Problem 9.6}

Batch normalization improves training speed.

\begin{enumerate}
    \item Does it change the function class?
    \item Why does it still change optimization behavior?
\end{enumerate}

Explain carefully.

\vspace{4cm}

%=================================================
\subsection*{Problem Set D: Generalization}
%=================================================

\textbf{Problem 9.7}

A neural network fits training data perfectly.

\begin{enumerate}
    \item Why might test error still be low?
    \item What properties of the solution matter?
\end{enumerate}

Relate your answer to flat minima.

\vspace{4cm}

%-
\textbf{Problem 9.8}

Explain why dropout is more effective
in fully connected layers
than in the output layer.

What failure does it prevent?

\vspace{4cm}

%=================================================
\subsection*{Problem Set E: Counterexample Thinking}
%=================================================

\textbf{Problem 9.9}

Give a scenario where:
\begin{itemize}
    \item A deeper network generalizes worse than a shallower one
\end{itemize}

Explain:
\begin{enumerate}
    \item The data
    \item The architecture
    \item The failure mechanism
\end{enumerate}

\vspace{5cm}

%-
\textbf{Problem 9.10}

Explain why the following statement is misleading:

\begin{quote}
``ReLU networks do not suffer from vanishing gradients.''
\end{quote}

When can ReLU still fail?

\vspace{4cm}

%=================================================
\subsection*{Problem Set F: Synthesis}
%=================================================

\textbf{Problem 9.11 (Very Important)}

Explain how the following interact:

\begin{itemize}
    \item Initialization
    \item Activation functions
    \item Learning rate
    \item Normalization
\end{itemize}

Why must they be designed together?

\vspace{5cm}

%=================================================
\subsection*{Final Reflection (Expert Check)}
%=================================================

\textbf{Problem 9.12}

Write a short paragraph answering:

\begin{quote}
Why do neural networks work in practice despite
non-convex optimization and massive overparameterization?
\end{quote}

If your answer does not mention optimization bias,
representation, or flat minima,
revisit Chapters 6--8.

\vspace{6cm}

%=================================================
\section{Answers — Chapter 9}
%=================================================

\textbf{Answer 9.1}

Better representation can reduce bias;
implicit regularization can control variance.

%-
\textbf{Answer 9.2}

Each ReLU introduces a region split.
Depth composes splits.

%-
\textbf{Answer 9.3}

Optimization method and data.

%-
\textbf{Answer 9.4}

Deep networks are harder to optimize
but more expressive.

%-
\textbf{Answer 9.5}

Gradients decay multiplicatively.
Data does not affect this.

%-
\textbf{Answer 9.6}

Function class unchanged.
Optimization landscape improved.

%-
\textbf{Answer 9.7}

Flat minima generalize better.

%-
\textbf{Answer 9.8}

Prevents co-adaptation in hidden units.

%-
\textbf{Answer 9.9}

Small dataset, deep model,
high variance.

%-
\textbf{Answer 9.10}

Dead ReLUs and poor initialization.

%-
\textbf{Answer 9.11}

They jointly control signal and gradient flow.

%-
\textbf{Answer 9.12}

Optimization favors simple representations.
\end{document}