\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}

\geometry{margin=1in}

\title{\textbf{Machine Learning}\\
(Geometry $\rightarrow$ Probability $\rightarrow$ Optimization)}
\author{}
\date{}

\begin{document}

\maketitle

\tableofcontents
\newpage

%-------------------------------------------------

\section{Machine Learning}

\subsection*{Big Picture (Before Any Model)}

\textbf{Machine Learning = choosing a function from data}

At its heart, machine learning answers a very simple question:

\begin{quote}
Given past examples, how should we predict future outcomes?
\end{quote}

Mathematically, this process is:
\[
\text{Data} \;\longrightarrow\; \text{Model } f_\theta \;\longrightarrow\; \text{Prediction}
\]

Here:
\begin{itemize}
    \item \textbf{Data} are observed examples
    \item $f_\theta$ is a function with parameters $\theta$
    \item \textbf{Prediction} is the model’s output on new data
\end{itemize}

\paragraph{Learning = Optimization}

Learning means choosing parameters $\theta$ so that predictions are as accurate as possible.
This is expressed as an optimization problem:
\[
\min_\theta \; \text{Loss(Data, } f_\theta)
\]

The loss function measures \textbf{how wrong} the model is.

\paragraph{Four Mathematical Pillars}

Every machine learning model relies on the same four mathematical ideas:
\begin{itemize}
    \item \textbf{Linear Algebra} $\rightarrow$ how data and models are represented
    \item \textbf{Probability} $\rightarrow$ how uncertainty and noise are modeled
    \item \textbf{Calculus} $\rightarrow$ how models are adjusted (learning)
    \item \textbf{Optimization} $\rightarrow$ how best parameters are chosen
\end{itemize}

\subsubsection*{ Bridge — What Is Really Happening?}

Think of machine learning as:
\begin{quote}
Adjusting knobs on a machine until its output matches reality as closely as possible.
\end{quote}

The knobs are parameters $\theta$.  
The adjustment rule comes from mathematics.

%-------------------------------------------------

\section{Module 5.1 — Supervised Learning (Foundations)}

\subsection{What ``Supervised'' Really Means}

In supervised learning, we are given examples where the correct answer is already known.

Formally:
\[
(x_i, y_i), \quad i = 1,\dots,n
\]

\begin{itemize}
    \item $x_i$: input (feature vector describing the object)
    \item $y_i$: output (correct label or value)
\end{itemize}

\paragraph{Goal}

Learn a function:
\[
f(x) \approx y
\]

so that it works well not only on known data, but also on \textbf{new unseen data}.

\paragraph{Two Fundamental Tasks}

\begin{itemize}
    \item \textbf{Regression}: output is a real number ($y \in \mathbb{R}$)
    \item \textbf{Classification}: output belongs to a finite set of classes
\end{itemize}

\subsubsection*{ Bridge — Why Supervision Matters}

Supervision means:
\begin{quote}
The teacher tells us the correct answer during learning.
\end{quote}

This feedback is what allows optimization to work.

%-------------------------------------------------

\section{Part A — Linear Regression (The Core of ML)}

Linear regression is the simplest supervised learning model.
Almost every advanced model can be understood as a modification of it.

Understanding linear regression deeply means understanding machine learning.

%-------------------------------------------------

\subsection{5.1.1 Simple Linear Regression (One Variable)}

\subsubsection*{Problem Setup}

We observe data points:
\[
(x_1,y_1),\dots,(x_n,y_n)
\]

Each $x_i$ is a single real number.

\paragraph{Model}

We assume a linear relationship:
\[
y = \beta_0 + \beta_1 x
\]

\begin{itemize}
    \item $\beta_0$: intercept (value of $y$ when $x=0$)
    \item $\beta_1$: slope (rate of change of $y$ with respect to $x$)
\end{itemize}

\subsubsection*{ Bridge — Why a Line?}

A line is the \textbf{simplest smooth relationship}.
If even a line cannot fit the data reasonably,
no complex model should be trusted.

%-------------------------------------------------

\subsubsection*{Geometry First}

Each data point lies somewhere in the plane.
The line tries to pass as close as possible to all points.

The error for point $i$ is:
\[
e_i = y_i - (\beta_0 + \beta_1 x_i)
\]

This error is a \textbf{vertical distance} from the line.

%-------------------------------------------------

\subsubsection*{Loss Function (Why Squared Error?)}

We measure total error using:
\[
L(\beta_0,\beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
\]

\paragraph{Why squares?}
\begin{enumerate}
    \item Positive and negative errors do not cancel
    \item Large mistakes are penalized heavily
    \item The function is smooth and differentiable
\end{enumerate}

\subsubsection*{ Bridge — Physical Analogy}

Think of each squared error as \textbf{energy}.
The best line is the one with minimum total energy.

%-------------------------------------------------

\subsubsection*{Optimization (Calculus Enters)}

To minimize the loss, we differentiate with respect to parameters:
\[
\frac{\partial L}{\partial \beta_0} = 0, \quad
\frac{\partial L}{\partial \beta_1} = 0
\]

This yields the \textbf{normal equations}:
\[
\begin{aligned}
\sum y_i &= n\beta_0 + \beta_1 \sum x_i \\
\sum x_i y_i &= \beta_0 \sum x_i + \beta_1 \sum x_i^2
\end{aligned}
\]

Solving:
\[
\beta_1 = \frac{\text{Cov}(x,y)}{\text{Var}(x)}, \quad
\beta_0 = \bar{y} - \beta_1 \bar{x}
\]

\paragraph{Interpretation}

Slope measures how strongly $x$ and $y$ vary together.

%-------------------------------------------------

\subsubsection*{Probabilistic Interpretation}

Assume the data follows:
\[
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]

where:
\[
\varepsilon_i \sim \mathcal{N}(0,\sigma^2)
\]

Then:
\begin{itemize}
    \item Squared loss comes from Gaussian likelihood
    \item Least squares equals maximum likelihood estimation
\end{itemize}

\subsubsection*{ Bridge — Why Gaussian Noise?}

Gaussian noise arises naturally when many small disturbances act together
(Central Limit Theorem).

%-------------------------------------------------

\subsubsection*{GATE Traps}

\begin{itemize}
    \item Correlation does not imply causation
    \item Regression does not prove cause–effect
    \item Intercept is not optional
\end{itemize}

%-------------------------------------------------

\subsection{5.1.2 Multiple Linear Regression (Vector Form)}

Now inputs have multiple features:
\[
x_i \in \mathbb{R}^d
\]

\paragraph{Model}

\[
y = \beta_0 + \beta_1 x_1 + \dots + \beta_d x_d
\]

Each feature contributes linearly.

%-------------------------------------------------

\subsubsection*{Matrix Representation (Crucial)}

Define:
\[
X =
\begin{bmatrix}
1 & x_{11} & \dots & x_{1d} \\
1 & x_{21} & \dots & x_{2d} \\
\vdots & \vdots & & \vdots \\
1 & x_{n1} & \dots & x_{nd}
\end{bmatrix},
\quad
\beta =
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_d
\end{bmatrix}
\]

Prediction:
\[
\hat{y} = X\beta
\]

Loss:
\[
L(\beta) = \|y - X\beta\|^2
\]

\subsubsection*{ Bridge — Why Matrices?}

Matrices allow us to:
\begin{quote}
Write many equations as one geometric statement.
\end{quote}

%-------------------------------------------------

\subsubsection*{Geometry (The Heart)}

\begin{itemize}
    \item $y$ is a vector in $\mathbb{R}^n$
    \item Columns of $X$ span a subspace
    \item $X\beta$ is the projection of $y$ onto that subspace
\end{itemize}

The error vector is orthogonal to the column space.

%-------------------------------------------------

\subsubsection*{Normal Equation}

Orthogonality gives:
\[
X^T X \beta = X^T y
\]

If $X^TX$ is invertible:
\[
\beta = (X^T X)^{-1} X^T y
\]

\subsubsection*{Key Linear Algebra Facts}

\begin{itemize}
    \item $X^TX$ is symmetric and positive semi-definite
    \item Rank deficiency implies infinite solutions
    \item Pseudoinverse gives minimum-norm solution
\end{itemize}
%-------------------------------------------------

\subsubsection*{Normal Equation}

\[
X^T X \beta = X^T y
\]

If invertible:
\[
\beta = (X^T X)^{-1} X^T y
\]

%-------------------------------------------------

\subsubsection*{Key Linear Algebra Facts}

\begin{itemize}
    \item $X^T X$ is symmetric and positive semi-definite
    \item Rank deficiency implies infinite solutions
    \item Moore--Penrose pseudoinverse resolves singularity
\end{itemize}

%-------------------------------------------------

\subsection{5.1.3 Ridge Regression (Regularization)}

\subsubsection*{Why Do We Need Ridge Regression?}

Ordinary Least Squares (OLS) regression works well when data is clean and features are independent.
However, in real datasets, three major problems arise:

\begin{itemize}
    \item \textbf{Multicollinearity}: features are highly correlated
    \item \textbf{Overfitting}: model fits noise instead of signal
    \item \textbf{Numerical instability}: $X^TX$ is nearly singular
\end{itemize}

These issues cause regression coefficients to become very large and unstable.

\subsubsection*{ Bridge — What Is Going Wrong in OLS?}

Geometrically, OLS tries to project $y$ onto the column space of $X$.

If columns of $X$ are nearly linearly dependent:
\begin{itemize}
    \item The column space is almost flat in some directions
    \item Tiny changes in data cause large changes in $\beta$
\end{itemize}

Ridge regression stabilizes this geometry.

%-------------------------------------------------

\subsubsection*{Modified Loss Function}

Ridge regression adds a penalty on the size of coefficients:

\[
L(\beta) = \|y - X\beta\|^2 + \lambda \|\beta\|^2
\]

where:
\begin{itemize}
    \item $\|y - X\beta\|^2$ measures data fitting error
    \item $\|\beta\|^2 = \beta^T\beta$ measures model complexity
    \item $\lambda > 0$ controls the strength of regularization
\end{itemize}

\subsubsection*{ Bridge — Why Penalize $\|\beta\|^2$?}

Large coefficients mean the model is:
\begin{itemize}
    \item Extremely sensitive to small data changes
    \item Likely to overfit
\end{itemize}

Penalizing $\|\beta\|^2$ prefers:
\begin{quote}
Simpler models that explain data without extreme parameter values.
\end{quote}

%-------------------------------------------------

\subsubsection*{Closed-Form Solution}

Minimizing the ridge loss yields:
\[
\beta = (X^TX + \lambda I)^{-1} X^T y
\]

\paragraph{Why This Always Works}

\begin{itemize}
    \item $X^TX$ is positive semi-definite
    \item $\lambda I$ makes $X^TX + \lambda I$ strictly positive definite
    \item Therefore, the matrix is always invertible
\end{itemize}

\subsubsection*{Interpretation}

\begin{itemize}
    \item Coefficients are shrunk toward zero
    \item Variance decreases, bias increases slightly
    \item Overall generalization improves
\end{itemize}

This is the \textbf{bias–variance trade-off in action}.

%-------------------------------------------------

\subsubsection*{Geometric Interpretation}

OLS minimizes distance to $y$ using elliptical contours.

Ridge regression introduces a circular constraint:
\begin{itemize}
    \item OLS solution lies at the ellipse minimum
    \item Ridge solution lies where ellipse meets circle
\end{itemize}

This forces coefficients to remain small.

\subsubsection*{ Bridge — One-Dimensional Example}

If $y \approx 100x$ but data is noisy:
\begin{itemize}
    \item OLS chooses slope $\approx 100$
    \item Ridge chooses a smaller slope to avoid overreaction
\end{itemize}

%-------------------------------------------------

\subsubsection*{GATE Traps (Ridge Regression)}

\begin{itemize}
    \item Ridge does \textbf{not} perform feature selection
    \item Regularization parameter $\lambda$ must be positive
    \item Ridge changes the solution even when OLS exists
\end{itemize}

%=================================================

\subsection{5.1.4 Logistic Regression (Classification)}

In classification problems, outputs belong to classes:
\[
y \in \{0,1\}
\]

Linear regression fails here because:
\begin{itemize}
    \item Predictions are unbounded
    \item Squared loss is inappropriate for probabilities
\end{itemize}

Logistic regression fixes this using probability theory.

%-------------------------------------------------

\subsubsection*{Model}

Logistic regression models the probability of class $1$ as:
\[
P(y=1|x) = \sigma(w^T x)
\]

where the sigmoid function is:
\[
\sigma(z) = \frac{1}{1+e^{-z}}
\]

\subsubsection*{ Bridge — Score vs Probability}

\begin{itemize}
    \item $w^Tx$ is a \textbf{score} (any real number)
    \item Sigmoid converts score into probability
\end{itemize}

%-------------------------------------------------

\subsubsection*{Why Sigmoid Function?}

The sigmoid function:
\begin{itemize}
    \item Maps $\mathbb{R} \rightarrow (0,1)$
    \item Is smooth and differentiable
    \item Increases monotonically
\end{itemize}

This makes it ideal for probabilistic classification.

\subsubsection*{ Bridge — Physical Interpretation}

Think of sigmoid as a soft switch:
\begin{itemize}
    \item Large positive input $\rightarrow$ almost 1
    \item Large negative input $\rightarrow$ almost 0
\end{itemize}

%-------------------------------------------------

\subsubsection*{Loss Function (Log-Likelihood)}

Assuming Bernoulli outcomes:
\[
P(y_i|x_i) = p_i^{y_i}(1-p_i)^{1-y_i}
\]

Log-likelihood:
\[
\ell(w) = \sum_{i=1}^n
\left[
y_i \log p_i + (1-y_i)\log(1-p_i)
\right]
\]

Negative log-likelihood gives the loss:
\[
L(w) = -\ell(w)
\]

This is known as \textbf{cross-entropy loss}.

\subsubsection*{ Bridge — Why Not Squared Error?}

Squared error treats classification as regression.
Cross-entropy treats classification as \textbf{probability estimation},
which is mathematically correct.

%-------------------------------------------------

\subsubsection*{Optimization}

There is no closed-form solution because the loss is nonlinear.

We use gradient descent:
\[
w^{(t+1)} = w^{(t)} - \eta \nabla L(w)
\]

where:
\begin{itemize}
    \item $\eta$ is the learning rate
    \item $\nabla L(w)$ points toward steepest increase
\end{itemize}

\subsubsection*{ Bridge — Why Gradient Descent Works}

The loss surface is convex.
Any descent direction leads to the global minimum.

%-------------------------------------------------

\subsubsection*{Geometry of Logistic Regression}

\begin{itemize}
    \item Decision boundary: $w^T x = 0$
    \item Boundary is a hyperplane
    \item Classification depends on which side of the plane $x$ lies
\end{itemize}

\subsubsection*{ Bridge — Linear Boundary, Probabilistic Output}

\begin{itemize}
    \item Geometry decides the class boundary
    \item Probability decides confidence
\end{itemize}

%-------------------------------------------------

\subsubsection*{GATE Traps (Logistic Regression)}

\begin{itemize}
    \item Logistic regression is a \textbf{classification} algorithm
    \item Output is a probability, not a label
    \item Loss is log-likelihood, not squared error
\end{itemize}

%=================================================
\section{Module 5.2 — k-Nearest Neighbours and Naive Bayes}
%=================================================

This module contrasts two fundamentally different learning philosophies:

\begin{itemize}
    \item \textbf{k-NN}: learning by geometry and similarity
    \item \textbf{Naive Bayes}: learning by probability and uncertainty
\end{itemize}

Understanding this contrast builds deep intuition and is frequently tested in GATE.

%-------------------------------------------------

\subsection{5.2.1 k-Nearest Neighbours (k-NN)}

\subsubsection*{Problem Statement}

Given a new data point $x^{*}$, predict its label using the most similar points from the training data.

k-NN has:
\begin{itemize}
    \item No explicit training phase
    \item No parameter estimation
    \item No loss function to minimize
\end{itemize}

Hence, it is called a \textbf{lazy learning algorithm}.

\subsubsection*{ Bridge — Why Is This Still “Learning”?}

Even though k-NN does not build a model,
it still learns in the following sense:

\begin{quote}
The data itself \emph{is} the model.
\end{quote}

All intelligence is postponed until prediction time.

%-------------------------------------------------

\subsubsection*{Geometry First ( )}

Each data point is a point in a $d$-dimensional feature space $\mathbb{R}^d$.

Similarity between points is defined using a distance metric.
Prediction depends entirely on the \textbf{local neighbourhood} around $x^{*}$.

\subsubsection*{ Bridge — Intuition}

Points that are close together in feature space are assumed to have similar outputs.
This assumption is called the \textbf{smoothness assumption}.

%-------------------------------------------------

\subsubsection*{Mathematical Formulation}

Training data:
\[
\{(x_i,y_i)\}_{i=1}^n, \quad x_i \in \mathbb{R}^d
\]

The most common distance measure is Euclidean distance:
\[
\|x - x_i\|_2 = \sqrt{(x-x_i)^T(x-x_i)}
\]

\paragraph{Prediction Rule (Classification)}

\[
\hat{y}(x^{*}) = \text{majority label among the } k \text{ nearest neighbours}
\]

\paragraph{Prediction Rule (Regression)}

\[
\hat{y}(x^{*}) = \frac{1}{k}\sum_{i \in \mathcal{N}_k(x^{*})} y_i
\]

\subsubsection*{ Bridge — Why Averaging Works}

Averaging nearby outputs smooths noise and approximates the underlying function locally.

%-------------------------------------------------

\subsubsection*{Linear Algebra Insight}

Distance depends on the inner product:
\[
\|x - x_i\|^2 = (x-x_i)^T(x-x_i)
\]

This reveals two important facts:
\begin{itemize}
    \item Distance is sensitive to feature scaling
    \item Inner products behave poorly in very high dimensions
\end{itemize}

\subsubsection*{Curse of Dimensionality}

As dimension $d$ increases:
\begin{itemize}
    \item All points become almost equally distant
    \item Neighbourhoods lose meaning
\end{itemize}

This phenomenon is called the \textbf{curse of dimensionality}.

\subsubsection*{ Bridge — Why k-NN Fails in High Dimensions}

In high dimensions:
\begin{quote}
``Nearest'' and ``farthest'' neighbours are almost the same distance away.
\end{quote}

Hence, similarity loses its meaning.

%-------------------------------------------------

\subsubsection*{Bias--Variance Behaviour}

\begin{center}
\begin{tabular}{|c|c|}
\hline
$k$ value & Behaviour \\
\hline
Small $k$ & Low bias, high variance \\
Large $k$ & High bias, low variance \\
\hline
\end{tabular}
\end{center}

Extreme cases:
\begin{itemize}
    \item $k=1$: perfect memorization (overfitting)
    \item $k=n$: predicts global mean or majority (underfitting)
\end{itemize}

\subsubsection*{ Bridge — Choosing $k$}

Choosing $k$ is a direct application of the bias--variance trade-off.

%-------------------------------------------------

\subsubsection*{Algorithm}

Given a test point $x^{*}$:
\begin{enumerate}
    \item Compute distance to all training points
    \item Sort distances
    \item Select $k$ nearest neighbours
    \item Perform voting (classification) or averaging (regression)
\end{enumerate}

Time complexity:
\[
O(nd) \quad \text{per query}
\]

\subsubsection*{ Bridge — Why k-NN Is Slow}

All computation happens at prediction time.
There is no compression of data into parameters.

%-------------------------------------------------

\subsubsection*{Probabilistic Interpretation}

k-NN approximates conditional probability locally:
\[
P(y|x) \approx
\frac{\text{number of neighbours of class } y}{k}
\]

Thus, k-NN acts as a \textbf{non-parametric density estimator}.

\subsubsection*{ Bridge — Geometry Meets Probability}

Local neighbour counts approximate probability density without assuming any distribution.

%-------------------------------------------------

\subsubsection*{GATE Traps (k-NN)}

\begin{itemize}
    \item k-NN has no training loss function
    \item Feature scaling dramatically affects performance
    \item k-NN does not learn parameters
    \item Performance degrades rapidly in high dimensions
\end{itemize}
%=================================================

\subsection{5.2.2 Naive Bayes Classifier}

\subsubsection*{Philosophy}

Naive Bayes answers a very simple but powerful question:

\begin{quote}
Given what I observe, which class is most probable?
\end{quote}

Unlike k-NN, Naive Bayes does not rely on geometry or distance.
It relies entirely on \textbf{probability theory}.

It is called a \textbf{generative model} because it models how data is generated
inside each class.

\subsubsection*{ Bridge — What Does “Generative” Mean?}

Generative means:
\begin{quote}
The model learns how each class generates data.
\end{quote}

Once this is known, classification is done by comparing probabilities.

%-------------------------------------------------

\subsubsection*{Bayes Theorem}

Bayes theorem connects cause and effect:
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
\]

Here:
\begin{itemize}
    \item $P(y)$ is the prior belief about class $y$
    \item $P(x|y)$ is the likelihood of observing $x$ given class $y$
    \item $P(y|x)$ is the posterior probability
\end{itemize}

Since $P(x)$ is common to all classes:
\[
\hat{y} = \arg\max_y P(x|y)P(y)
\]

\subsubsection*{ Bridge — Why Can We Ignore $P(x)$?}

For classification, we only care about \textbf{which class is larger},
not the exact probability value.

%-------------------------------------------------

\subsubsection*{Naive Conditional Independence Assumption}

Naive Bayes assumes:
\[
P(x_1,\dots,x_d \mid y)
=
\prod_{j=1}^d P(x_j \mid y)
\]

This says:
\begin{quote}
Once the class is known, features behave independently.
\end{quote}

\subsubsection*{ Bridge — Why Is This “Naive”?}

In reality, features are often correlated.
Yet Naive Bayes works because:
\begin{itemize}
    \item Errors cancel out across features
    \item Classification only depends on relative probabilities
\end{itemize}

%-------------------------------------------------

\subsubsection*{Decision Rule}

To avoid numerical underflow, we take logarithms:
\[
\hat{y}
=
\arg\max_y
\left[
\log P(y) + \sum_{j=1}^d \log P(x_j|y)
\right]
\]

\subsubsection*{Why Logarithms?}

\begin{itemize}
    \item Products become sums
    \item Small probabilities do not underflow
    \item Computation becomes stable
\end{itemize}

%-------------------------------------------------

\subsubsection*{Variants of Naive Bayes}

\paragraph{Gaussian Naive Bayes}

Assumes continuous features:
\[
x_j|y \sim \mathcal{N}(\mu_{y,j},\sigma_{y,j}^2)
\]

\paragraph{Bernoulli Naive Bayes}

Assumes binary features:
\[
x_j \in \{0,1\}
\]

\paragraph{Multinomial Naive Bayes}

Assumes count data (e.g.\ word frequencies).

\subsubsection*{ Bridge — Same Idea, Different Data}

Only the likelihood model changes.
Bayes theorem remains the same.

%-------------------------------------------------

\subsubsection*{Geometry vs Probability}

\begin{itemize}
    \item Decision boundaries arise from probability ratios
    \item No distances or inner products are used
    \item Boundaries may be linear or quadratic
\end{itemize}

\subsubsection*{Bias--Variance Behaviour}

\begin{center}
\begin{tabular}{|c|c|}
\hline
Aspect & Naive Bayes \\
\hline
Bias & High \\
Variance & Low \\
Data requirement & Very small \\
\hline
\end{tabular}
\end{center}

\subsubsection*{ Bridge — Why It Works with Small Data}

Strong assumptions reduce variance.
Low variance means stability with limited data.

%-------------------------------------------------

\subsubsection*{GATE Traps (Naive Bayes)}

\begin{itemize}
    \item Conditional independence $\neq$ independence
    \item Naive Bayes is generative, not discriminative
    \item Performs well even when assumptions fail
    \item Bayes theorem is used explicitly
\end{itemize}

%=================================================

\subsection*{k-NN vs Naive Bayes (Exam Comparison)}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Aspect & k-NN & Naive Bayes \\
\hline
Type & Non-parametric & Parametric \\
Training & None & Required \\
Core math & Geometry & Probability \\
Scaling sensitivity & High & Low \\
Small data & Poor & Excellent \\
\hline
\end{tabular}
\end{center}

%=================================================
\section{Module 5.3 — Linear Discriminant Analysis and Support Vector Machines}
%=================================================

This module represents the deepest geometric ideas in Machine Learning.

\begin{itemize}
    \item \textbf{LDA}: probability determines geometry
    \item \textbf{SVM}: geometry alone determines separation
\end{itemize}

%=================================================
\subsection{5.3.1 Linear Discriminant Analysis (LDA)}
%=================================================

\subsubsection*{Problem Setting}

We are given labeled data:
\[
(x_i, y_i), \quad x_i \in \mathbb{R}^d, \quad y_i \in \{1,2,\dots,C\}
\]

Goal:
\begin{quote}
Find a projection where classes are maximally separated.
\end{quote}

%-------------------------------------------------

\subsubsection*{Probabilistic Assumptions}

LDA assumes:
\begin{enumerate}
    \item Class-conditional distributions are Gaussian
    \item All classes share the same covariance matrix $\Sigma$
    \item Class priors may differ
\end{enumerate}

\subsubsection*{ Bridge — Why Gaussian?}

When many small effects combine, Gaussian distributions naturally arise
(Central Limit Theorem).

%-------------------------------------------------

\subsubsection*{Bayes Classification Rule}

Bayes theorem:
\[
P(y=k|x) \propto P(x|y=k)P(y=k)
\]

Gaussian likelihood:
\[
\log P(x|y=k)
=
-\frac{1}{2}(x-\mu_k)^T \Sigma^{-1} (x-\mu_k)
-\frac{1}{2}\log|\Sigma|
\]

Ignoring constants:
\[
\delta_k(x)
=
x^T \Sigma^{-1} \mu_k
-
\frac{1}{2}\mu_k^T \Sigma^{-1} \mu_k
+
\log P(y=k)
\]

Classification:
\[
\hat{y} = \arg\max_k \delta_k(x)
\]

%-------------------------------------------------

\subsubsection*{Geometry of LDA}

Decision boundary between classes $k$ and $l$:
\[
\delta_k(x) = \delta_l(x)
\Rightarrow w^T x + b = 0
\]

Thus:
\begin{itemize}
    \item LDA yields linear decision boundaries
    \item Direction depends on $\Sigma^{-1}(\mu_k-\mu_l)$
\end{itemize}

\subsubsection*{ Bridge — Probability Shapes Geometry}

The inverse covariance matrix shrinks directions with high variance
and emphasizes stable directions.

%-------------------------------------------------

\subsubsection*{Two-Class LDA (Core Case)}

Class means:
\[
\mu_1,\quad \mu_2
\]

Within-class scatter:
\[
S_W =
\sum_{x_i \in \mathcal{C}_1} (x_i-\mu_1)(x_i-\mu_1)^T
+
\sum_{x_i \in \mathcal{C}_2} (x_i-\mu_2)(x_i-\mu_2)^T
\]

Between-class scatter:
\[
S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T
\]

%-------------------------------------------------

\subsubsection*{Optimization View (Rayleigh Quotient)}

Projection vector $w$ maximizes:
\[
J(w) = \frac{w^T S_B w}{w^T S_W w}
\]

Solution:
\[
w \propto S_W^{-1}(\mu_1 - \mu_2)
\]

This is a generalized eigenvalue problem.

\subsubsection*{ Bridge — What Is Being Maximized?}

Numerator separates class means.
Denominator penalizes class spread.
LDA balances both.

%-------------------------------------------------

\subsubsection*{Key Linear Algebra Insights}

\begin{itemize}
    \item $S_W$ must be invertible
    \item Regularization may be needed
    \item Projection maximizes separability
\end{itemize}

%-------------------------------------------------

\subsubsection*{GATE Traps (LDA)}

\begin{itemize}
    \item LDA is both generative and discriminative
    \item Equal covariance assumption is critical
    \item Decision boundaries are linear
\end{itemize}

%=================================================
\subsection{5.3.2 Support Vector Machines (SVM)}
%=================================================

\subsubsection*{Philosophy}

Support Vector Machines answer one geometric question:

\begin{quote}
Among all separating hyperplanes, which one separates the data
\emph{most confidently}?
\end{quote}

Confidence is measured by the \textbf{margin}.

SVM makes:
\begin{itemize}
    \item No probabilistic assumptions
    \item No distributional assumptions
\end{itemize}

It is a \textbf{purely geometric algorithm}.

\subsubsection*{ Bridge — Why Margin Matters}

A classifier that barely separates points is fragile.
A classifier with a large margin is stable to noise and perturbations.

Margin is robustness.

%-------------------------------------------------

\subsubsection*{Linear Separability}

We are given labeled data:
\[
(x_i, y_i), \quad y_i \in \{-1,+1\}
\]

A hyperplane in $\mathbb{R}^d$ is defined as:
\[
w^T x + b = 0
\]

This hyperplane splits space into two half-spaces.

Correct classification requires:
\[
y_i(w^T x_i + b) \ge 1
\]

\subsubsection*{ Bridge — Why Use $\pm 1$ Labels?}

Using $\{-1,+1\}$ makes geometry symmetric
and simplifies mathematical expressions.

%-------------------------------------------------

\subsubsection*{Margin Geometry}

The distance from a point $x$ to the hyperplane is:
\[
\text{distance} = \frac{|w^T x + b|}{\|w\|}
\]

For support vectors:
\[
|w^T x + b| = 1
\]

Hence, the margin width is:
\[
\frac{2}{\|w\|}
\]

\subsubsection*{Key Insight}

Maximizing the margin is equivalent to minimizing $\|w\|^2$.

\subsubsection*{ Bridge — Why Minimize $\|w\|^2$?}

A smaller $\|w\|$ means:
\begin{itemize}
    \item The separating plane is flatter
    \item The margin is wider
    \item The classifier is more robust
\end{itemize}

%-------------------------------------------------

\subsubsection*{Primal Optimization Problem}

The hard-margin SVM solves:
\[
\min_{w,b} \; \frac{1}{2}\|w\|^2
\]

subject to:
\[
y_i(w^T x_i + b) \ge 1, \quad \forall i
\]

\subsubsection*{Why This Is a Good Problem}

\begin{itemize}
    \item Objective is convex
    \item Constraints are linear
    \item Global minimum is guaranteed
\end{itemize}

This is a \textbf{convex quadratic optimization problem}.

%-------------------------------------------------

\subsubsection*{Lagrangian Formulation}

To solve constrained optimization, we form the Lagrangian:
\[
\mathcal{L}(w,b,\alpha)
=
\frac{1}{2}\|w\|^2
-
\sum_{i=1}^n \alpha_i [y_i(w^T x_i + b)-1]
\]

where $\alpha_i \ge 0$ are Lagrange multipliers.

\subsubsection*{ Bridge — What Are Lagrange Multipliers Doing?}

They enforce constraints while allowing unconstrained optimization.

Only constraints that are ``tight'' matter.

%-------------------------------------------------

\subsubsection*{Optimality Conditions}

Taking derivatives and setting to zero gives:
\[
w = \sum_{i=1}^n \alpha_i y_i x_i
\]

This shows:
\begin{quote}
The classifier is a weighted sum of training points.
\end{quote}

%-------------------------------------------------

\subsubsection*{Dual Optimization Problem}

Substituting back yields the dual problem:
\[
\max_{\alpha}
\sum_{i=1}^n \alpha_i
-
\frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j x_i^T x_j
\]

subject to:
\[
\alpha_i \ge 0, \quad \sum_i \alpha_i y_i = 0
\]

\subsubsection*{Why Dual Form Matters}

\begin{itemize}
    \item Depends only on inner products
    \item Enables kernel methods
    \item Often easier to solve numerically
\end{itemize}

%-------------------------------------------------

\subsubsection*{Support Vectors}

Only points with $\alpha_i > 0$ contribute to $w$.

These points lie:
\begin{itemize}
    \item Exactly on the margin
    \item Or violate it (soft margin case)
\end{itemize}

They are called \textbf{support vectors}.

\subsubsection*{ Bridge — Why Only Few Points Matter}

Interior points do not affect the margin.
Only boundary points define the classifier.

%-------------------------------------------------

\subsubsection*{Soft Margin SVM}

Real data is rarely perfectly separable.

Introduce slack variables $\xi_i$:
\[
\min_{w,b,\xi} \;
\frac{1}{2}\|w\|^2 + C\sum_i \xi_i
\]

subject to:
\[
y_i(w^T x_i + b) \ge 1 - \xi_i,
\quad \xi_i \ge 0
\]

\subsubsection*{Role of $C$}

\begin{itemize}
    \item Large $C$ $\rightarrow$ low bias, high variance
    \item Small $C$ $\rightarrow$ higher bias, lower variance
\end{itemize}

This is bias–variance trade-off.

%-------------------------------------------------

\subsubsection*{Kernel Trick (Inner Product View)}

Replace inner products:
\[
x_i^T x_j \rightarrow K(x_i,x_j)
\]

This allows nonlinear decision boundaries in the original space.

\subsubsection*{ Bridge — The Miracle}

\begin{quote}
We act as if data lives in high dimensions,
without ever computing those dimensions.
\end{quote}

Only inner products are needed.

%-------------------------------------------------

\subsubsection*{GATE Traps (SVM)}

\begin{itemize}
    \item SVM maximizes margin, not accuracy
    \item Only support vectors define the classifier
    \item Kernel replaces inner product
    \item Convexity guarantees global optimum
\end{itemize}

%=================================================
\subsection*{LDA vs SVM (Exam Comparison)}
%=================================================

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Aspect & LDA & SVM \\
\hline
Philosophy & Probability-based & Geometry-based \\
Assumptions & Gaussian & None \\
Boundary & Linear & Linear / Nonlinear \\
Optimization & Eigenvalue problem & Quadratic programming \\
Small data & Good & Moderate \\
\hline
\end{tabular}
\end{center}
%=================================================
\section{Module 5.4 — Dimensionality Reduction and Principal Component Analysis}
%=================================================

Modern datasets often have many features.
Some are redundant, noisy, or irrelevant.

Dimensionality reduction answers the question:

\begin{quote}
What are the most important directions in which the data varies?
\end{quote}

Principal Component Analysis (PCA) is the most fundamental method to answer this.

%=================================================
\subsection{5.4.1 Why Dimensionality Reduction?}
%=================================================

High-dimensional data causes multiple problems:
\begin{itemize}
    \item Computational inefficiency
    \item Overfitting
    \item Curse of dimensionality
    \item Difficulty in visualization
\end{itemize}

The goal is to represent data using \textbf{fewer dimensions}
while preserving as much information as possible.

\subsubsection*{ Bridge — Real Meaning}

Dimensionality reduction is:
\begin{quote}
Finding the few directions that explain most of what is happening.
\end{quote}

%=================================================
\subsection{5.4.2 Data Representation}
%=================================================

Let the data matrix be:
\[
X =
\begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_n^T
\end{bmatrix}
\in \mathbb{R}^{n \times d}
\]

Each row is a data point in $\mathbb{R}^d$.

Before PCA:
\begin{itemize}
    \item Center the data
\end{itemize}

\[
\tilde{X} = X - \mathbf{1}\mu^T
\]

where $\mu$ is the mean vector.

\subsubsection*{ Bridge — Why Center the Data?}

PCA studies \textbf{variation}, not absolute location.
Centering removes the effect of origin.

%=================================================
\subsection{5.4.3 Variance as Geometry}
%=================================================

Variance measures how spread out data is.

For a direction $w$:
\[
\text{Variance along } w = \frac{1}{n} \sum_{i=1}^n (w^T x_i)^2
\]

Using matrix notation:
\[
\text{Var}(w) = w^T S w
\]

where $S$ is the covariance matrix.

%=================================================
\subsection{5.4.4 Covariance Matrix}
%=================================================

The sample covariance matrix is:
\[
S = \frac{1}{n} \tilde{X}^T \tilde{X}
\]

Properties:
\begin{itemize}
    \item Symmetric
    \item Positive semi-definite
\end{itemize}

\subsubsection*{ Bridge — What Covariance Means}

Covariance tells us:
\begin{quote}
Which directions change together, and how strongly.
\end{quote}

%=================================================
\subsection{5.4.5 PCA Optimization Problem}
%=================================================

PCA seeks a unit vector $w$ maximizing variance:
\[
\max_{w} \; w^T S w
\quad \text{subject to } \|w\| = 1
\]

This is a constrained optimization problem.

\subsubsection*{Solution}

Using Lagrange multipliers:
\[
S w = \lambda w
\]

Thus:
\begin{itemize}
    \item Principal components are eigenvectors of $S$
    \item Variance equals eigenvalues
\end{itemize}

\subsubsection*{ Bridge — Why Eigenvectors Appear}

Eigenvectors are directions that remain unchanged
except for scaling.
They are the \textbf{natural axes of variation}.

%=================================================
\subsection{5.4.6 Multiple Principal Components}
%=================================================

The first principal component maximizes variance.

Subsequent components:
\begin{itemize}
    \item Are orthogonal to previous ones
    \item Capture remaining variance
\end{itemize}

Choosing top $k$ components gives:
\[
Z = \tilde{X} W_k
\]

where $W_k$ contains top $k$ eigenvectors.

%=================================================
\subsection{5.4.7 PCA as Projection}
%=================================================

PCA is a projection:
\[
x \mapsto W_k^T x
\]

It gives the \textbf{best rank-$k$ approximation}
in the least-squares sense.

\subsubsection*{ Bridge — Best Approximation}

Among all $k$-dimensional subspaces,
PCA minimizes reconstruction error.

%=================================================
\subsection{5.4.8 PCA via Singular Value Decomposition (SVD)}
%=================================================

Compute SVD of centered data:
\[
\tilde{X} = U \Sigma V^T
\]

Then:
\begin{itemize}
    \item Columns of $V$ are principal directions
    \item Singular values relate to variance
\end{itemize}

\subsubsection*{ Bridge — Why SVD is Preferred}

\begin{itemize}
    \item Numerically stable
    \item Works even when $d > n$
    \item Avoids explicit covariance computation
\end{itemize}

%=================================================
\subsection{5.4.9 Choosing Number of Components}
%=================================================

Explained variance ratio:
\[
\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i}
\]

Choose $k$ such that:
\begin{itemize}
    \item Most variance is preserved
    \item Noise is discarded
\end{itemize}

%=================================================
\subsection{5.4.10 PCA Limitations}
%=================================================

\begin{itemize}
    \item PCA is linear
    \item Sensitive to scaling
    \item Does not use labels
\end{itemize}

\subsubsection*{ Bridge — When PCA Fails}

If data lies on a nonlinear manifold,
linear projections are insufficient.

%=================================================
\subsection{5.4.11 GATE Traps (PCA)}
%=================================================

\begin{itemize}
    \item PCA maximizes variance, not class separation
    \item Eigenvectors of covariance, not data matrix
    \item Centering is mandatory
    \item PCA is unsupervised
\end{itemize}
%=================================================
\section{Module 5.5 — Clustering Algorithms}
%=================================================

Clustering is an unsupervised learning problem.

Unlike supervised learning:
\begin{itemize}
    \item There are no labels
    \item There is no teacher
\end{itemize}

The goal is to discover \textbf{hidden structure} in data.

\begin{quote}
Which data points naturally belong together?
\end{quote}

%=================================================
\subsection{5.5.1 What Is Clustering?}
%=================================================

Given data points:
\[
\{x_1, x_2, \dots, x_n\}, \quad x_i \in \mathbb{R}^d
\]

Clustering partitions the data into groups (clusters) such that:
\begin{itemize}
    \item Points in the same cluster are similar
    \item Points in different clusters are dissimilar
\end{itemize}

\subsubsection*{ Bridge — Human Intuition}

Humans cluster naturally:
\begin{itemize}
    \item Similar colors
    \item Similar shapes
    \item Similar behaviors
\end{itemize}

Clustering tries to formalize this intuition mathematically.

%=================================================
\subsection{5.5.2 k-Means Clustering}
%=================================================

\subsubsection*{Objective of k-Means}

k-means aims to partition data into $k$ clusters
by minimizing within-cluster variance.

Mathematically:
\[
\min_{\{\mathcal{C}_1,\dots,\mathcal{C}_k\}}
\sum_{j=1}^k \sum_{x_i \in \mathcal{C}_j}
\|x_i - \mu_j\|^2
\]

where $\mu_j$ is the centroid of cluster $\mathcal{C}_j$.

\subsubsection*{ Bridge — What Is Being Minimized?}

k-means tries to:
\begin{quote}
Pull points as close as possible to their cluster centers.
\end{quote}

This is purely a geometric objective.

%-------------------------------------------------

\subsubsection*{Centroid Update Rule}

Given cluster $\mathcal{C}_j$, the optimal centroid is:
\[
\mu_j = \frac{1}{|\mathcal{C}_j|} \sum_{x_i \in \mathcal{C}_j} x_i
\]

\subsubsection*{Why the Mean?}

The mean minimizes squared distance.
This follows from setting derivatives to zero.

%=================================================
\subsection{5.5.3 k-Means Algorithm}
%=================================================

\begin{enumerate}
    \item Initialize $k$ centroids randomly
    \item Assign each point to the nearest centroid
    \item Update centroids as cluster means
    \item Repeat until convergence
\end{enumerate}

\subsubsection*{Convergence}

Each step:
\begin{itemize}
    \item Never increases the objective
    \item Decreases or keeps it constant
\end{itemize}

Hence, k-means converges to a \textbf{local minimum}.

\subsubsection*{ Bridge — Why Only Local Minimum?}

The objective is non-convex.
Initialization determines the final solution.

%=================================================
\subsection{5.5.4 Geometry of k-Means}
%=================================================

\begin{itemize}
    \item Clusters are Voronoi regions
    \item Decision boundaries are linear
    \item Each point belongs to nearest centroid
\end{itemize}

\subsubsection*{Relation to PCA}

k-means often works better after PCA:
\begin{itemize}
    \item Noise is reduced
    \item Distance becomes meaningful
\end{itemize}

%=================================================
\subsection{5.5.5 k-Medoid (Robust Variant)}
%=================================================

Unlike k-means:
\begin{itemize}
    \item Cluster center must be a data point
    \item Uses arbitrary distance metrics
\end{itemize}

k-medoid is more robust to outliers.

%=================================================
\subsection{5.5.6 Limitations of k-Means}
%=================================================

\begin{itemize}
    \item Requires $k$ beforehand
    \item Sensitive to initialization
    \item Assumes spherical clusters
    \item Sensitive to scaling
\end{itemize}

\subsubsection*{GATE Traps (k-Means)}

\begin{itemize}
    \item k-means minimizes squared distances
    \item Centroid is mean, not median
    \item Converges to local optimum
\end{itemize}

%=================================================
\subsection{5.5.7 Hierarchical Clustering}
%=================================================

Hierarchical clustering builds a hierarchy of clusters.

Two main types:
\begin{itemize}
    \item Agglomerative (bottom-up)
    \item Divisive (top-down)
\end{itemize}

%=================================================
\subsection{5.5.8 Agglomerative Clustering}
%=================================================

Algorithm:
\begin{enumerate}
    \item Start with each point as a cluster
    \item Merge closest clusters
    \item Repeat until one cluster remains
\end{enumerate}

Result is a \textbf{dendrogram}.

\subsubsection*{ Bridge — What Is a Dendrogram?}

A dendrogram is a tree showing
how clusters merge at different distances.

%=================================================
\subsection{5.5.9 Linkage Criteria}
%=================================================

Distance between clusters can be defined as:

\begin{itemize}
    \item \textbf{Single linkage}: minimum distance
    \item \textbf{Complete linkage}: maximum distance
    \item \textbf{Average linkage}: mean distance
\end{itemize}

Different linkages produce different cluster shapes.

%=================================================
\subsection{5.5.10 Geometry of Hierarchical Clustering}
%=================================================

\begin{itemize}
    \item No cluster center is required
    \item Arbitrary cluster shapes allowed
    \item Sensitive to noise (especially single linkage)
\end{itemize}

%=================================================
\subsection{5.5.11 Comparison: k-Means vs Hierarchical}
%=================================================

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Aspect & k-Means & Hierarchical \\
\hline
Need $k$ & Yes & No \\
Scalability & High & Low \\
Cluster shape & Spherical & Arbitrary \\
Initialization & Required & None \\
\hline
\end{tabular}
\end{center}

%=================================================
\subsection{5.5.12 GATE Traps (Clustering)}
%=================================================

\begin{itemize}
    \item Clustering is unsupervised
    \item k-means uses Euclidean distance
    \item Hierarchical clustering is greedy
    \item Dendrogram cut defines final clusters
\end{itemize}
%=================================================
\section{Module 5.6 — Bias--Variance Trade-off and Cross-Validation}
%=================================================

This module explains the most important question in Machine Learning:

\begin{quote}
Why does a model work well on training data but fail on new data?
\end{quote}

Understanding this module means you truly understand learning.

%=================================================
\subsection{5.6.1 What Does It Mean to Learn?}
%=================================================

We observe data generated as:
\[
y = f(x) + \varepsilon
\]

where:
\begin{itemize}
    \item $f(x)$ is the true (unknown) function
    \item $\varepsilon$ is random noise
\end{itemize}

Our model $\hat{f}(x)$ tries to approximate $f(x)$.

\subsubsection*{ Bridge — The Reality Check}

We never see $f(x)$ directly.
We only see noisy samples.
Learning means guessing the pattern behind noise.

%=================================================
\subsection{5.6.2 Expected Prediction Error}
%=================================================

Consider the expected squared error at a point $x$:
\[
\mathbb{E}[(y - \hat{f}(x))^2]
\]

Substituting $y = f(x) + \varepsilon$:
\[
\mathbb{E}[(f(x) + \varepsilon - \hat{f}(x))^2]
\]

This decomposes into three terms.

%=================================================
\subsection{5.6.3 Bias--Variance Decomposition}
%=================================================

The expected error decomposes as:
\[
\mathbb{E}[(y - \hat{f}(x))^2]
=
\underbrace{(\mathbb{E}[\hat{f}(x)] - f(x))^2}_{\text{Bias}^2}
+
\underbrace{\mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]}_{\text{Variance}}
+
\underbrace{\sigma^2}_{\text{Noise}}
\]

\subsubsection*{Meaning of Each Term}

\begin{itemize}
    \item \textbf{Bias}: error from wrong assumptions
    \item \textbf{Variance}: sensitivity to training data
    \item \textbf{Noise}: irreducible randomness
\end{itemize}

\subsubsection*{ Bridge — Physical Analogy}

\begin{itemize}
    \item Bias: aiming at the wrong target
    \item Variance: shaky hand
    \item Noise: wind you cannot control
\end{itemize}

%=================================================
\subsection{5.6.4 Bias and Underfitting}
%=================================================

High bias models:
\begin{itemize}
    \item Are too simple
    \item Miss important patterns
    \item Underfit the data
\end{itemize}

Example:
\begin{itemize}
    \item Linear regression on nonlinear data
\end{itemize}

%=================================================
\subsection{5.6.5 Variance and Overfitting}
%=================================================

High variance models:
\begin{itemize}
    \item Are overly flexible
    \item Fit noise instead of signal
    \item Perform poorly on new data
\end{itemize}

Example:
\begin{itemize}
    \item Deep decision trees
    \item k-NN with small $k$
\end{itemize}

%=================================================
\subsection{5.6.6 Bias--Variance Trade-off}
%=================================================

Increasing model complexity:
\begin{itemize}
    \item Decreases bias
    \item Increases variance
\end{itemize}

Decreasing complexity:
\begin{itemize}
    \item Increases bias
    \item Decreases variance
\end{itemize}

\subsubsection*{ Bridge — The Balance}

There is no perfect model.
The best model balances bias and variance.

%=================================================
\subsection{5.6.7 Bias--Variance Behavior of Common Models}
%=================================================

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Model & Bias & Variance \\
\hline
Linear Regression & High & Low \\
Ridge Regression & Moderate & Low \\
k-NN ($k$ small) & Low & High \\
Naive Bayes & High & Low \\
Decision Tree (deep) & Low & Very High \\
SVM (large margin) & Moderate & Low \\
\hline
\end{tabular}
\end{center}

%=================================================
\subsection{5.6.8 Training Error vs Test Error}
%=================================================

\begin{itemize}
    \item Training error always decreases with complexity
    \item Test error first decreases, then increases
\end{itemize}

This produces the classic U-shaped curve.

\subsubsection*{ Bridge — Why Training Error Lies}

A model can memorize training data without learning the pattern.

%=================================================
\subsection{5.6.9 Cross-Validation}
%=================================================

Cross-validation estimates how a model will perform on unseen data.

%-------------------------------------------------
\subsubsection*{Hold-Out Validation}

Split data into:
\begin{itemize}
    \item Training set
    \item Validation set
\end{itemize}

Problem:
\begin{itemize}
    \item Wasteful
    \item High variance estimate
\end{itemize}

%-------------------------------------------------
\subsubsection*{$k$-Fold Cross-Validation}

Procedure:
\begin{enumerate}
    \item Split data into $k$ folds
    \item Train on $k-1$ folds
    \item Test on remaining fold
    \item Average results
\end{enumerate}

Estimated error:
\[
\hat{E}_{CV} = \frac{1}{k} \sum_{i=1}^k E_i
\]

\subsubsection*{ Bridge — Why CV Works}

Each data point gets to be both:
\begin{itemize}
    \item Training data
    \item Test data
\end{itemize}

%-------------------------------------------------
\subsubsection*{Leave-One-Out Cross-Validation (LOO)}

Special case: $k = n$.

\begin{itemize}
    \item Very low bias
    \item Very high variance
    \item Computationally expensive
\end{itemize}

%=================================================
\subsection{5.6.10 Model Selection}
%=================================================

Cross-validation is used to:
\begin{itemize}
    \item Choose hyperparameters
    \item Compare models
    \item Control overfitting
\end{itemize}

The final model is retrained on full data.

%=================================================
\subsection{5.6.11 GATE Traps (Bias--Variance \& CV)}
%=================================================

\begin{itemize}
    \item Bias--variance is about expected error
    \item Training error is not a good metric
    \item CV estimates test error, not training error
    \item LOO has high variance
\end{itemize}
\end{document}